#+TITLE:    Space-Efficiently Dynamic: How to Grow Hash Tables without Doubling their Size
#+AUTHOR:   Tobias Maier
#+EMAIL:    t.maier@kit.edu

** Abstract
A lot of research has been done to optimize the speed and space
efficiency of hash tables.  Additionally many practical
implementations exist.  Modern displacement strategies can fill a
given table to more than 95% before the performance begins to decrease
significantly.  But once the threshold is met, copying and migrating
the whole table still seems to be the most efficient answer.

To achieve any kind of efficiency, each such migration must be
amortized by newly inserted elements.  Therefore, each migration has
to increase the capacity by a significant amount -- usually, this
means doubling the capacity.  Growing by any reasonable factor
$\gamma$ (e.g. 2) leads to bad space efficiency because the newly grown table
is only filled to less than a factor of $\gamma^{-1}$ ($\frac12$).

The space efficiency is even worse during the migration, where both
the new and the old table coexist at the same time causing a massive
space overhead.  To solve both of these issues, we propose the
following two level hash table that supports dynamic growing while keeping a
strict size constraint $\alpha \cdot n$) throughout its lifetime.

* Introduction
Hash tables are some of the most frequently used data-structures. They
belong into every programmers basic toolbox.  As such having
interchangeable implementations which perform well under different
conditions and circumstances is important and allows programmers to
worry about different problems.

One aspect which a lot of research focuses on is space efficiency.
Modern space efficient hash tables work well up to loads of 90\% and
more. But for this to matter, programmers have to know similarly tight
bounds to the absolute number of elements inserted into the table.
This is typically not the case, one of the most ubiquitous use cases
for hash tables is to unify data connected to the same key.  To
guarantee good performance, we have to overestimate the necessary
capacity whenever the exact number of unique keys is not known a
priori.  To alleviate this problem even in circumstances where space
efficiency is important, space efficiently growing hash tables are
necessary.

Many libraries -- even ones that implement space efficient hash tables
-- already offer dynamic growing.  The problem with these
implementations is that they either lose their space efficiency or
their overall performance, once the table grows above its original
capacity.  Usually growing is implemented by either creating
additional hash tables -- decreasing the performance especially for
look ups, or by migrating all elements to a new table -- losing the
space efficiency by multiplying the original size.

To avoid both of these pitfalls we propose a variant of bucket cuckoo
hashing (TODO REFERENCE) that uses multiple subtables each of which
can grow independent from all others, alleviating pressure from all
tables.  Each subtable is accessible through a global first level
table.  Bucket cuckoo hashing is a technique where each hashed element
can be stored in one of several possible buckets (groups of cells),
these buckets are spread over the subtables making it possible to move
elements between subtables.


#+BEGIN_COMMENT
The second level tables consist of Buckets which actually store the
elements.  While the table is not growing, operations proceed similar
to normal cuckoo hashing.  Each element correlates to two buckets
(each within one subtable).  These buckets depend on the hash function
in the following way: first we use the most significant bits of the
hash value to choose the appropriate subtable, then we use the lesser
significant bits to find a location within that subtable.  When
inserting an element we will store it in one of its correlating
buckets.  If there is no space in the connected buckets, then we use
common cuckoo displacement techniques to make room.
#+END_COMMENT

Doubling the size of one subtable increases the overall size by a
small factor, while only moving a small number of elements -- making
it possible, to amortize these small size changes. The introduced size
and occupancy imbalance between subtables is alleviated using
displacement techniques common to cuckoo hashing. Using these
techniques, makes our table work efficiently with fill degrees of
... .  Being approximately ... % faster than similar hash table
techniques under the same circumstances.

#+BEGIN_COMMENT
When the table needs to grow, one can double the size of a subtable,
increasing the overall capacity only by a small factor.  Only the
elements within the grown subtable are migrated. Therefore the
additional space is linear in the number of migrated elements which is
important for the amortization. The so grown subtable now
contains twice the number of buckets, as before. This introduces some
imbalance, because the percentage of elements that hash to one
subtable does not depend on the subtable size.  But we will show that
using a breadth-first-search displacement technique will in practice
find short displacement paths that lead to a successful insertion.

Therefore, our dynamic space efficient tables work efficiently even
when filled to similar degrees as homogeneous tables using the same
displacement strategy. They also work up to ... times better than
other space efficient approaches.
#+END_COMMENT

* Motivation
The goal in evolving hash tables has long been to make hash tables
work better in high load scenarios.  Modern hash table techniques like
Hopscotch, Robin Hood, and Cuckoo Hashing can fill hash tables to more
than 90% while still achieving constant average running times on most
of their operations.

But this only makes sense, if the exact number of unique elements that
are inserted into the table is known a priori. To visualize this
assume the following scenario.  During the operation of a word count
benchmark, we know there will be at most $c$ unique words. Therefore,
we construct a table and initialize it with $1.1\cdot c$
size. Therefore, the resulting table is below $1.1^{-1} = 91\%$
filled, and thus can operate effectively. Let us now assume, the
specific word count instance only contained $\gamma \cdot c$ unique
words.  Then the table is only filled to ...\% which means that it takes up

* Related Work
- Some other engineering papers

- Some Space efficient Stuff  (theoretical)

- Some Space efficient Stuff  (practical)

* Preliminaries

A hash table is a data-structure, which stores key-value-pairs
(\(\langle key, data \rangle\)) and offers the following functions:
~insert~ stores a given key-value pair, ~find~ given a key returns the
stored value or false if the key is not stored, and ~remove~ which
deletes a previously inserted element (if present).

An alternative model which is sometimes used in literature considers
arbitrary elements instead of key value pairs.  In that model, the key
is extracted using an extractor function.  All techniques presented in
this paper also work in the extractor model without any adaptation.

Throughout this paper, we use $n$ to denote the number of elements and
$m$ the number of cells ($m > n$) in any given hash table.  We define
the load factor as $load = n/m$.  Tables can usually only work
efficiently up to a certain load factor.  Above that, operations get
slower and/or have a possibility to fail.
# Is this really OPEN vs. CLOSED or DIRECT vs. INDIRECT
When implementing a hash table one has to decide between storing
elements directly in the hash table -- /Open Addressing/ -- or storing
pointers to elements -- /Closed Hashing/. This has an immediate impact
on the necessary memory.
\[mem_{open}  (n,m) \geq^* m\cdot size(element)\]
\[mem_{closed}(n,m) \geq^* m\cdot size(pointer) + n\cdot size(element)\]

For large elements (\(>> size(pointer)\)), one can simply use closed
hashing, to reduce the relevant memory factor.  Therefore, we restrict
ourselves to the common case of elements whose size is comparable to
that of a pointer.  For our experiments we use 128bit elements (64bit
keys and 64bit values).  In these instances closed addressing
introduces a significant memory overhead, therefore, we only consider
open addressing hash tables for the purpose of this paper. For open
addressing hash tables, the memory efficiency is directly dependent on
the element density.

#+BEGIN_COMMENT
Let $n$ be the number of elements in a hash table which has $m$ cells.
Sometimes we combine multiple cells into buckets, then $b$ will denote
the bucket size (number of cells in one bucket).  When allocating the
table, we allocate it with approximately $n\cdot \alpha$ cells.
Therefore, an appropriately filled table has \(m=n\cdot\alpha\).

We call /se/ the size of one element.  For most hash tables this is
also the size of each hash table cell.  Therefore, the size of the hash
table is usually \(/se/\cdot m\).

We call a hash table architecture *Open Addressing*, when it stores
its elements directly in the table.  This is the opposite of indirect
hash table techniques like hashing with chaining. Where the hash table
usually contains pointers to elements that are stored outside the main
table.  In this publication, we concentrate fully on hash tables with
open addressing since indirect tables cannot be space efficient when
the element size is comparable to the pointer size of the used system
(for an explanation see TODO).
#+END_COMMENT

** Cuckoo Hashing
Cuckoo hashing is a technique to resolve hash conflicts in an open
addressing hash table. Its main draw is that it guarantees constant
lookup times even in densely filled tables. The article TODO by TODO
(probably mitzenmacher with or without dietzfelbinger) et al. gives a
good overview over many cuckoo hashing variants.  The distinguishing
technique of cuckoo hashing is, that $k$ hash functions (\(h_1, ... ,
h_k\)) are used to find $k$ independent possible positions. Each
element is stored in one of its positions.  Even if all possible
positions are occupied one can often move elements to open space for the
current element.

Bucket cuckoo hashing is a variant, where the cells of the hash table
are grouped into buckets of size $b$.  Each element appointed to one
bucket can be stored in any of its cells ($m/b = \textit{number of
buckets}$).  Using buckets one can increase the number of displacement
opportunities significantly.

*Lookup:* Find and Remove operations have a guaranteed constant
running time. Independent from the tables density, there are $k$
possible buckets -- $k\cdot b$ cells -- that have to be searched to
find an element.

*Insert:* Each element is hashed to with $k$ buckets.  If at least
one of those buckets has space left, then we store the element in the
bucket, that has the most space.  But when both buckets are full, then we
have to move elements around the table, such that a new space becomes
available.

# IMPLICIT GRAPH MODEL
To visualize the problem of displacing elements, one can think of the
directed graph implicitly defined by the hash table.  Each bucket
defines one node and each element defines pairwise connections between
the bucket it is stored in to its $k-1$ alternate buckets.  To insert
an element into the hash table we have to find a path from one of its
associated buckets, to a bucket, that has space remaining.  Then we
move elements along this path, to open a space on the initial bucket.
The two common techniques to find such paths are /random walks/ and
/breadth first searches/.

*** Some Performance Bounds

** $\alpha$-Space Efficient Hash Tables
We call a hash table \(\alpha\)-space efficient when it can work
efficiently using $\alpha\cdot n\cdot size(element) + constant$ memory
(TODO DEFINE WORKING EFFICIENTLY FURTHER i.e. constant average running
time or just look at experiments or inserting the last 10% elements is
$~$ to inserting all others).  In many classic open addressing
techniques -- like linear probing and cuckoo hashing -- cells are
exactly as large as the stored elements. Therefore, being $\alpha$
space efficient is the same, as being able to operate efficiently
under a load factor of $\alpha^{-1} = n/m$ ($\alpha\cdot n \cdot
size(element) = m\cdot size(element)$).

This is not necessarily the case for every kind of hashing
technique. The memory footprint can be larger, especially when we keep
additional per cell or per bucket information. This is for example the
case for hopscotch hashing, where each cell stores some additional
neighborhood data.  This would also be the case if we store some per
bucket data like the number of stored elements in our bucket cuckoo
hash table.

#+BEGIN_COMMENT
In the absence of compression, it is clear that the minimum memory
required for storing a hash table is the combined memory of all stored
elements \((/se/ \cdot n)\).  We call a hash table
$\alpha$-space efficient -- for an $1\leq \alpha$ when it can efficiently operate
with $n$ elements while using less than \(\alpha \cdot n \cdot
/se/ + c\) memory.

While using open addressing hash tables it is clear, that we cannot
reach the minimum space of \(n\cdot/se/\) in a non-static scenario
with on-line insertions and deletions.  Therefore, we assume that
\(\alpha > 0\).  To efficiently use the given memory, we will usually
make sure, that the used tables are \(m = \alpha \cdot n\) cells large
resulting in the necessary memory consumption.

Note that above statement is only true for tables which use open
addressing.  It disregards indirect techniques which store pointers in
the table.  This makes sense in the common scenario of reasonably
small elements (not \(/se/ >> /pointer size/\)). In this scenario,
storing one pointer per element becomes prohibitively large
(impossible for any $\alpha < \frac{/se/ + /pointer size/}{/se/}$).

In the case where elements are large compared to pointers we can
easily construct a hash table with good space efficiency by using a
non-space efficient table and storing only pointers to elements.
Using this technique the overall memory consumption is \(\approx
m\cdot /pointer size/ + n\cdot /se/\). Which is close to the optimum
\(n\cdot /se/\) for large /se/.

Therefore, we concentrate on hash tables with open addressing. This
means, that all investigated hash tables at some level consist of an
array, which stores elements directly.
#+END_COMMENT

*** Why is Dynamic Size Important!
Let us assume we use an $\alpha$-space efficient hash table. We know
an upper bound $c$ to the number of unique elements used in our test.
If the specific instant uses only $n = \gamma\cdot c$ elements
($\gamma < 1$), then good results could have been achieved using a
\(\alpha\cdot 1/\gamma\%\) space efficient table that was properly
initiated. Therefore, optimizing static tables to support higher
densities only makes sense if the expected size of the table can be
guessed equally exact.

For example looking at a classic cuckoo hash table with ....TODO. This
table works well until it is approximately 95% filled ($\alpha =
0.95^{-1}$), therefore we choose \(m = 0.95^{-1}\cdot u\) where $u$ is
the known upper bound to the number of elements.  If the instance
contains 20% less unique keys than the upper bound, the space
consumption is \(m\cdot /se/ = 0.95^{-1}\cdot u \cdot /se/ =
0.95^{-1}\cdot 0.8^{-1}\cdot n \cdot /se/ \approx 1.32 \cdot n \cdot
/se/\).

To achieve space efficiency in cases where the final table size is
unknown or only known approximately, we have to remain space
efficient at all times during the hash tables life span (every
operation could be the last).  Therefore, the size of the table has to
grow closely together with the number of inserted elements -- all
while ensuring that the costs of each operation stay (expected,
amortized) constant.

This leads us to hash tables, that can be space efficient while they
grow dynamically.

*** $\alpha$-Space Efficiency for Dynamic Tables
The definition of a space efficient table specifically works for
preallocated tables. Even an empty table can be space
efficient, if it can still operate when it is filled further. It has
to be initialized with an exact count of unique keys, to actually make
the $\alpha$-space efficiency matter. Since this is not
necessarily possible we will now define the notion of
space efficient dynamic tables.

We call a hash table $\alpha$-space efficiently growing,
when it can grow over its original capacity and throughout its
lifetime, will only use \(\alpha\cdot n^* \cdot /se/\) where
$n^*$ is the *peek* number of elements (observed maximum).

**** Trivial Examples
One simple example of this is a linear probing table, which grows by a
factor of two, whenever 50% of the table is filled. Here the table is
at least 25% filled (immediately after growing), therefore, the table
is 4-space efficient. Another example would be a cuckoo table which
doubles in size whenever it surpasses 95%, fill rate. This would be a
$0.45^{-1}\approx 2.22$-space efficient table.

**** Memory Usage while Growing
An additional problem we have not yet mentioned is the memory usage,
during table migrations. When we allocate a new table to move all
elements into, there is a time when both the old and the new table
coexist. during that time, the overall memory usage is even worse.  In
the example of the 4-space efficient linear probing table above there
is a time where \(6\cdot n \cdot /se/\) memory is used (old table 50%
filled and new table 0% filled). So as a dynamic table it is only 6-space
efficient.

* Two Level Data Structure
The main feature of our data structure is its ability to efficiently
grow with the number of elements stored in the table and thus remain
space efficient throughout its lifetime.  A common technique to make
static data structures dynamic  is to allocate a new structure with
twice the capacity and to migrate the currently held objects, whenever
the table gets too full. But doubling the whole data structure cannot
be space efficient.  The natural thought is to double only part of the
data structure.

** Description
We use a two level approach (shown in TODO FIGREFRENCE) to allow
differentiated growing between table parts.  The first level consists
of a table which stores pointers to /tl/ second level tables (\(/tl/ =
2^k\)). Each second level table consists of buckets that store
elements directly.

# Find the Buckets per element
Each element is still associated with exactly two buckets, these can
be in the same, or in different hash tables. To find the first bucket
associated with an element $e$, we compute $e$s hash value using the
first hash function $h_1(e)$. We then use the first $k$ bits, to chose
the appropriate second level table. There we find the specific bucket
using the other bits of the hash value (value modulo table size).

# Chaching of the first level Table
It is important to realize that the first level table will be cached,
whenever the table is accessed regularly. Therefore, our data
structure will cause a similar number of cache misses as any other
cuckoo hash table.  This allows our hierarchical table structure to be
competitive with statically sized growing cuckoo tables during non-growing
phases.

** Partial Growing
Let all second level tables start at the same size $s$e.  When the
number of elements surpasses a threshold ($\alpha \cdot n >
m+s$).  One can grow a second level table by doubling its
size and still remain space efficient.  Growing one table increases
the overall capacity from $/tl/\cdot s$ to $(/tl/+1)\cdot s$ (factor:
$\frac{/tl/+1}{/tl/}$).  Note that all subsequent grows increase the
capacity by the same absolute amount (smaller factor) until all second level
tables have the same size again (never grow a "big" table).

The table migration itself accesses cells in a linear fashion making
it very cache efficient.  The elements from each original bucket are
split onto two buckets of the target table.  Therefore, no bucket of
the target table can have more elements than its respective original
bucket -- guaranteeing that no displacements are necessary during the
migration.

Moreover, the cost of growing the subtable is amortized by element
insertions.  To trigger the growing $\alpha^{-1} \cdot s =
O(s)$ elements have to be inserted (globally).  One migration is guaranteed to
have $O(s)$ time, since, displacement is not needed during hash table
migrations (see above). Or from an abstract point of view, after
inserting $\alpha^{-1} \cdot s$ elements the table grows
enough, to raise the capacity about the same $\alpha^{-1} \cdot
s$ effective cells (actual cells $\times$ fill factor).

In the graph view of cuckoo tables (see section cuckoo hashing),
increasing the size of one table is equivalent to splitting each node
that represents a bucket within that table. Since the edges (elements)
are split between the nodes

** Imbalance between Second Level Tables
By growing individual subtables we introduce some imbalance into the
previously homogeneous table.  We use a constant number of hashed bits
to decide which subtables a given element can go into.  Therefore,
each subtable has approximately the same number of elements that are
associated with one of its buckets.  Therefore, it is not immediately
obvious, that increasing the size of one table will make it easier, to
insert elements that are not necessarily associated into that one
table.

Whenever we try to insert an element into the rest of the table and
there is no space for it, we start to move elements with one of the
common displacement strategies (random walk or breadth first search).
Each element we examine as part of the insertion has a chance to be
hashed into a bucket of the grown table.  Such an element will most
likely fit into its alternative bucket, since most buckets of the
grown table have free slots, thus the displacement will end.

** Shrinking
Some applications might require that the hash table can also shrink,
when elements get removed.  For example, when these elements are
reinserted into another data-structure (one grows the other shrinks).

Shrinking can work similarly to growing. We replace a second level
table with a smaller one, by migrating elements from one to the other.
During this migration elements from two buckets are joined into one
bucket. Therefore, it is possible for a bucket to be overfilled.
Whenever this is the case, we reinsert overfilled elements and let the
insertion algorithm figure out a redistribution in the table.

# There is an aggressivity parameter, which is used to regulate when to shrink the table.
Shrinking the table can only be amortized by remove operations when
$O(s)$ elements are removed before the table changes size -- counted
after the size change (growing or shrinking).  This can only work if
the table can temporarily remain larger than $\alpha \cdot n$ cells.
Otherwise there would be the possibility for quadratic behavior when
one element is repeatedly inserted and removed, if this one element
causes a grow/shrink.

** Potential Problems and Weaknesses
There are three factors, that impact the performance of our dynamic
table compared to other cuckoo tables and to hashing solutions in
general *inhomogeneous table resolution*, *element imbalance* and
*population density*. All of these factors influence the maximum load
density and the running times in different ways.

*** Inhomogeneous Table Resolution
When the table consists of different sized hash tables, it is hard to
spread the elements evenly among all available buckets.  But if the
elements are not spread evenly, the insertion time of an element can
vary depending on the second level hash tables the element in question
is hashed too. The problem might be, that not enough elements in the
small tables can be moved to the bigger tables to fill them to the
same degree as the smaller ones.

To show why spreading the elements can be hard, we look at the
following example.  If there are $n$ elements in a table with /tl/
second level tables, $k$ of which have size $2s$ the others have size
$s$. If elements are spread equally among cells then all small tables
have around $n/(/tl/ +k)$ elements, and the bigger tables have
$2n/(/tl/+k)$ elements. Each hash table has about $2n//tl/$ elements
associated to one of its buckets. For $k=1$ this means that nearly all
elements associated to a bucket in the first table would have to be
stored there, for the elements to be distributed evenly.

# Better if you use 3 hash functions

*** Element Imbalance
After a second level table grows there is a significant imbalance in
the way elements are spread over the available buckets.  The buckets
of the grown table have an average fill degree of less than 50% while
the other tables have a more even element distribution.  This effect
is only accentuated by the effects of inhomogeneous table resolution.
Since the newly grown table also has less associated elements per
bucket than the smaller tables.

Assume the following, all tables are filled completely when the first
table grows. Now there is new capacity for elements, but only in
the first second level table. So when we insert an element, we have to
find a way to move a previous element into the first table. When
inserting more and more elements, there are less and less elements
that can be moved into the first table.  This problem is intensified
by the observation made in (TODO previous thing about inhomogeneous
...). To fill the first table one would have to store about
$2n/(/tl/+1)$ elements there, which is hard because only about $2n//tl/$
of elements that is associated to buckets in that table.

# less severe if elements are deleted from the table  (grows slower) approaches random distribution

*** Population Density
Since our table grows with the number of elements, it is always
densely filled.  Thus, insertions naturally take longer than they
would if they were performed on an emptier table.

# This is kind of the point right? otherwise can still be initialized to be larger.

** Evaluating less Hash Functions
Evaluating a good hash function can take multiple CPU cycles, that can
usually not be overlaid with different tasks, because, their result is
important for the remaining operation (e.g. lookup find associated
buckets). Therefore, reducing the number of hash function computations
can save a lot of time.

To understand the following technique, we think about the potential
size of a hash table, and its number of buckets. Using $32$ bits one can
address $2^32$ buckets ($\approx 4\,$G) -- with a bucket size of eight
this makes $2^35\approx 34\,$G.  With a common element size of $64\,$bit per
key and $64\,$bit per value -- $16\,$Byte per pair -- the table has a size of
$512\,$GiB. This is big enough for any reasonable /single threaded/
application.  Therefore, instead of computing two hash functions, we
use one $64\,$bit hash function and split the result, to compute both
associated buckets.  For this to work, one has to use a hash function
which generates $64\,$bits of "randomness".

* Experiments
** Comparison Implementations
** Hardware
** Tests
*** Incremental Construction
*** Mixed Benchmarks
