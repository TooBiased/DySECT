#+TITLE:    Space-Efficiently Dynamic: How to Grow Hash Tables without Doubling their Size
#+AUTHOR:   Tobias Maier
#+EMAIL:    t.maier@kit.edu

** Abstract
A lot of research has been done to optimize the speed and space
efficiency of hash tables.  And many practical implementations exist.
Modern displacement strategies can easily fill a given table to more
than 95% before the performance begins to decrease significantly.  But
once the threshold is met, copying and migrating the table seems to
still be the most efficient answer.

During the migration the old and the new table coexist causing a
massive space overhead. Even if this was not the case and memory was
not an issue during the migration doubling the hash table size would
still lead to a bad space efficiency. Because, the newly grown table
is filled to at most 50%.

Therefore, we propose the following two level hash table that allows
dynamic growing and keeps a strict size constraint $\alpha$
\cdot n) throughout its lifetime.

* Introduction
Hash tables are some of the most frequently used data-structures. They
belong into every programmers basic toolbox.  As such having
implementations which perform well under different conditions is
important, to allow programmers to worry about different problems.

One aspect which a lot of research focuses on is space efficiency.
Modern space efficient hash tables work well up to loads of 90\% and
more. But for this to matter, programmers have to know similarly tight
bounds to the absolute number of elements inserted into the table.
This is often not the case, one of the most typical use cases for hash
tables is to unify data connected to keys.  To guarantee good
performance, we have to overestimate the necessary capacity whenever
the exact number of unique keys is not known a priori.  To
alleviate this problem, we now present a dynamically growing space
efficient hash table.

Many libraries -- even ones that implement space efficient hash tables
-- already offer dynamic growing.  The problem with these
implementations is that they either lose their space efficiency or
their overall performance, once the table grows above its original
capacity.  Usually growing is implemented by either creating
additional hash tables -- decreasing the performance especially for
look ups, or by migrating all elements to a new table -- losing the
space efficiency by multiplying the original size.

To avoid both of these pitfalls we propose the following variation of
a normal bucket cuckoo hash table.  We separate the global hash table
into a collection of smaller subtables.  There is a first level table,
which stores pointers to the second level subtables.  The second level
tables consist of Buckets which actually store the elements.  While
the table is not growing, operations proceed similar to normal cuckoo
hashing.  Each element correlates to two buckets (each within one
subtable).  These buckets depend on the hash function in the following
way: first we use the most significant bits of the hash value to
choose the appropriate subtable, then we use the lesser significant
bits to find a location within that subtable.  When inserting an
element we will store it in one of its correlating buckets.  If there
is no space in the connected buckets, then we use common cuckoo
displacement techniques to make room.

When the table needs to grow, one can double the size of a subtable,
increasing the overall capacity only by a small factor.  Only the
elements within the grown subtable are migrated. Therefore the
additional space is linear in the number of migrated elements which is
important for the amortization. The so grown subtable now
contains twice the number of buckets, as before. This introduces some
imbalance, because the percentage of elements that hash to one
subtable does not depend on the subtable size.  But we will show that
using a breadth-first-search displacement technique will in practice
find short displacement paths that lead to a successful insertion.

Therefore, our dynamic space efficient tables work efficiently even
when filled to similar degrees as homogeneous tables using the same
displacement strategy. They also work up to ... times better than
other space efficient approaches.

* Motivation

* Related Work
- Some other engineering papers

- Some Space efficient Stuff  (theoretical)

- Some Space efficient Stuff  (practical)

* Preliminaries


A hash table is a data-structure, which stores key-value-pairs
(\(\langle key, data \rangle\)) and offers the following functions:
~insert~ stores a given key-value pair, ~find~ given a key returns the
stored value or false if the key is not stored, and ~remove~ which
deletes a previously inserted element (if present).

An alternative model which is sometimes used in literature considers
arbitrary elements instead of key value pairs.  In that model, the key
is extracted using an extractor function.  All techniques presented in
this paper also work in the extractor model without any adaptation.

** Some Nomenclature
Let $n$ be the number of elements in a hash table which has $m$ cells.
Sometimes we combine multiple cells into buckets, then $b$ will denote
the bucket size (number of cells in one bucket).  When allocating the
table, we allocate it with approximately $n\cdot \alpha$ cells.
Therefore, an appropriately filled table has \(m=n\cdot\alpha\).

We call /se/ the size of one element.  For most hash tables this is
also the size of each hash table cell.  Therefore, the size of the hash
table is usually \(/se/\cdot m\).

We call a hash table architecture *Open Addressing*, when it stores
its elements directly in the table.  This is the opposite of indirect
hash table techniques like hashing with chaining. Where the hash table
usually contains pointers to elements that are stored outside the main
table.  In this publication, we concentrate fully on hash tables with
open addressing since indirect tables cannot be space efficient when
the element size is comparable to the pointer size of the used system
(for an explanation see TODO).

** Cuckoo Hashing
Cuckoo hashing is a technique to resolve hash conflicts in an open
addressing hash table. Its main draw is that it guarantees constant
lookup times even in densely filled tables. The article TODO by TODO
(probably mitzenmacher with or without dietzfelbinger) et al. gives a
good overview over the many variants of cuckoo hashing.  The
technique, which distinguishes variants of cuckoo hashing from most
other techniques is, that in case of conflicts one or more alternative
hash functions are used to find alternative positions.  Even when all
alternative positions are filled one can often move elements to open
up space for the current element.

Throughout this paper we use an adaptation of cuckoo hashing with
buckets. This variant was first presented in TODO ... .  It uses only
two hash functions, therefore, each element has exactly two buckets
that can store it.

*Lookup:* Search operations have a guaranteed constant running
time. Independent from the tables density, there are exactly two
possible buckets we have to search to find an element (each has
constant size).

*Insert:* Each element is associated with two buckets.  If at least
one of those buckets has space left, then we store the element in the
bucket, that has more space.  But when both buckets are full, then we
have to move elements around the table, such that a new space becomes
available.

# IMPLICIT GRAPH MODEL
To visualize the problem of displacing elements, one can think of the
following *implicit graph*.  There is one node per bucket and an edge
per element connecting its two associated buckets (nodes).  To insert
an element, we have to find a way from one of its associated buckets,
to a bucket, that has space remaining.  The two most common techniques
to do this is a *random walk* or a *breadth first search*.

*** Some Performance Bounds

** $\alpha$-Space Efficient Hash Tables
In the absence of compression, it is clear that the minimum memory
required for storing a hash table is the combined memory of all stored
elements \((/se/ \cdot n)\).  We call a hash table
$\alpha$-space efficient -- for an $1\leq \alpha$ when it can efficiently operate
with $n$ elements while using less than \(\alpha \cdot n \cdot
/se/ + c\) memory.

While using open addressing hash tables it is clear, that we cannot
reach the minimum space of \(n\cdot/se/\) in a non-static scenario
with on-line insertions and deletions.  Therefore, we assume that
\(\alpha > 0\).  To efficiently use the given memory, we will usually
make sure, that the used tables are \(m = \alpha \cdot n\) cells large
resulting in the necessary memory consumption.

Note that above statement is only true for tables which use open
addressing.  It disregards indirect techniques which store pointers in
the table.  This makes sense in the common scenario of reasonably
small elements (not \(/se/ >> /pointer size/\)). In this scenario,
storing one pointer per element becomes prohibitively large
(impossible for any $\alpha < \frac{/se/ + /pointer size/}{/se/}$).

In the case where elements are large compared to pointers we can
easily construct a hash table with good space efficiency by using a
non-space efficient table and storing only pointers to elements.
Using this technique the overall memory consumption is \(\approx
m\cdot /pointer size/ + n\cdot /se/\). Which is close to the optimum
\(n\cdot /se/\) for large /se/.

Therefore, we concentrate on hash tables with open addressing. This
means, that all investigated hash tables at some level consist of an
array, which stores elements directly.

*** Why is Dynamic Size Important!
Let us assume we use an $\alpha$-space efficient hash
table. We know an upper bound $c$ to the number of unique elements
used in our test.  If the specific instant uses only $n = \gamma\cdot
c$ elements ($\gamma < 1$), then good results could have been achieved
using a \(\alpha\cdot 1/\gamma\%\) space efficient table that
was properly initiated. Therefore, optimizing static tables to support
higher densities only makes sense if the expected size of the table
can be guessed equally exact.

For example looking at a classic cuckoo hash table with ....TODO. This
table works well until it is approximately 95% filled ($\alpha =
0.95^{-1}$), therefore we choose \(m = 0.95^{-1}\cdot u\) where $u$ is
the known upper bound to the number of elements.  If the instance
contains 20% less unique keys than the upper bound, the space
consumption is \(m\cdot /se/ = 0.95^{-1}\cdot u \cdot /se/ =
0.95^{-1}\cdot 0.8^{-1}\cdot n \cdot /se/ \approx 1.32 \cdot n \cdot
/se/\).

To achieve space efficiency in cases where the final table size is
unknown or only known approximately, we have to remain space
efficient at all times during the hash tables life span (every
operation could be the last).  Therefore, the size of the table has to
grow closely together with the number of inserted elements -- all
while ensuring that the costs of each operation stay (expected,
amortized) constant.

This leads us to hash tables, that can be space efficient while they
grow dynamically.

*** $\alpha$-Space Efficiency for Dynamic Tables
The definition of a space efficient table specifically works for
preallocated tables. Even an empty table can be space
efficient, if it can still operate when it is filled further. It has
to be initialized with an exact count of unique keys, to actually make
the $\alpha$-space efficiency matter. Since this is not
necessarily possible we will now define the notion of
space efficient dynamic tables.

We call a hash table $\alpha$-space efficiently growing,
when it can grow over its original capacity and throughout its
lifetime, will only use \(\alpha\cdot n^* \cdot /se/\) where
$n^*$ is the *peek* number of elements (observed maximum).

**** Trivial Examples
One simple example of this is a linear probing table, which grows by a
factor of two, whenever 50% of the table is filled. Here the table is
at least 25% filled (immediately after growing), therefore, the table
is 4-space efficient. Another example would be a cuckoo table which
doubles in size whenever it surpasses 95%, fill rate. This would be a
$0.45^{-1}\approx 2.22$-space efficient table.

**** Memory Usage while Growing
An additional problem we have not yet mentioned is the memory usage,
during table migrations. When we allocate a new table to move all
elements into, there is a time when both the old and the new table
coexist. during that time, the overall memory usage is even worse.  In
the example of the 4-space efficient linear probing table above there
is a time where \(6\cdot n \cdot /se/\) memory is used (old table 50%
filled and new table 0% filled). So as a dynamic table it is only 6-space
efficient.

* Two Level Data Structure
The main feature of our data structure is its ability to efficiently
grow with the number of elements stored in the table and thus remain
space efficient throughout its lifetime.  A common technique to make
static data structures dynamic  is to allocate a new structure with
twice the capacity and to migrate the currently held objects, whenever
the table gets too full. But doubling the whole data structure cannot
be space efficient.  The natural thought is to double only part of the
data structure.

** Description
We use a two level approach (shown in TODO FIGREFRENCE) to allow
differentiated growing between table parts.  The first level consists
of a table which stores pointers to /tl/ second level tables (\(/tl/ =
2^k\)). Each second level table consists of buckets that store
elements directly.

# Find the Buckets per element
Each element is still associated with exactly two buckets, these can
be in the same, or in different hash tables. To find the first bucket
associated with an element $e$, we compute $e$s hash value using the
first hash function $h_1(e)$. We then use the first $k$ bits, to chose
the appropriate second level table. There we find the specific bucket
using the other bits of the hash value (value modulo table size).

# Chaching of the first level Table
It is important to realize that the first level table will be cached,
whenever the table is accessed regularly. Therefore, our data
structure will cause a similar number of cache misses as any other
cuckoo hash table.  This allows our hierarchical table structure to be
competitive with statically sized growing cuckoo tables during non-growing
phases.

** Partial Growing
Let all second level tables start at the same size $s$e.  When the
number of elements surpasses a threshold ($\alpha \cdot n >
m+s$).  One can grow a second level table by doubling its
size and still remain space efficient.  Growing one table increases
the overall capacity from $/tl/\cdot s$ to $(/tl/+1)\cdot s$ (factor:
$\frac{/tl/+1}{/tl/}$).  Note that all subsequent grows increase the
capacity by the same absolute amount (smaller factor) until all second level
tables have the same size again (never grow a "big" table).

The table migration itself accesses cells in a linear fashion making
it very cache efficient.  The elements from each original bucket are
split onto two buckets of the target table.  Therefore, no bucket of
the target table can have more elements than its respective original
bucket -- guaranteeing that no displacements are necessary during the
migration.

Moreover, the cost of growing the subtable is amortized by element
insertions.  To trigger the growing $\alpha^{-1} \cdot s =
O(s)$ elements have to be inserted (globally).  One migration is guaranteed to
have $O(s)$ time, since, displacement is not needed during hash table
migrations (see above). Or from an abstract point of view, after
inserting $\alpha^{-1} \cdot s$ elements the table grows
enough, to raise the capacity about the same $\alpha^{-1} \cdot
s$ effective cells (actual cells $\times$ fill factor).

In the graph view of cuckoo tables (see section cuckoo hashing),
increasing the size of one table is equivalent to splitting each node
that represents a bucket within that table. Since the edges (elements)
are split between the nodes

** Imbalance between Second Level Tables
By growing individual subtables we introduce some imbalance into the
previously homogeneous table.  We use a constant number of hashed bits
to decide which subtables a given element can go into.  Therefore,
each subtable has approximately the same number of elements that are
associated with one of its buckets.  Therefore, it is not immediately
obvious, that increasing the size of one table will make it easier, to
insert elements that are not necessarily associated into that one
table.

Whenever we try to insert an element into the rest of the table and
there is no space for it, we start to move elements with one of the
common displacement strategies (random walk or breadth first search).
Each element we examine as part of the insertion has a chance to be
hashed into a bucket of the grown table.  Such an element will most
likely fit into its alternative bucket, since most buckets of the
grown table have free slots, thus the displacement will end.

** Shrinking
Some applications might require that the hash table can also shrink,
when elements get removed.  For example, when these elements are
reinserted into another data-structure (one grows the other shrinks).

Shrinking can work similarly to growing. We replace a second level
table with a smaller one, by migrating elements from one to the other.
During this migration elements from two buckets are joined into one
bucket. Therefore, it is possible for a bucket to be overfilled.
Whenever this is the case, we reinsert overfilled elements and let the
insertion algorithm figure out a redistribution in the table.

# There is an aggressivity parameter, which is used to regulate when to shrink the table.
Shrinking the table can only be amortized by remove operations when
$O(s)$ elements are removed before the table changes size -- counted
after the size change (growing or shrinking).  This can only work if
the table can temporarily remain larger than $\alpha \cdot n$ cells.
Otherwise there would be the possibility for quadratic behavior when
one element is repeatedly inserted and removed, if this one element
causes a grow/shrink.

** Potential Problems and Weaknesses
There are three factors, that impact the performance of our dynamic
table compared to other cuckoo tables and to hashing solutions in
general *inhomogeneous table resolution*, *element imbalance* and
*population density*. All of these factors influence the maximum load
density and the running times in different ways.

*** Inhomogeneous Table Resolution
When the table consists of different sized hash tables, it is hard to
spread the elements evenly among all available buckets.  But if the
elements are not spread evenly, the insertion time of an element can
vary depending on the second level hash tables the element in question
is hashed too. The problem might be, that not enough elements in the
small tables can be moved to the bigger tables to fill them to the
same degree as the smaller ones.

To show why spreading the elements can be hard, we look at the
following example.  If there are $n$ elements in a table with /tl/
second level tables, $k$ of which have size $2s$ the others have size
$s$. If elements are spread equally among cells then all small tables
have around $n/(/tl/ +k)$ elements, and the bigger tables have
$2n/(/tl/+k)$ elements. Each hash table has about $2n//tl/$ elements
associated to one of its buckets. For $k=1$ this means that nearly all
elements associated to a bucket in the first table would have to be
stored there, for the elements to be distributed evenly.

# Better if you use 3 hash functions

*** Element Imbalance
After a second level table grows there is a significant imbalance in
the way elements are spread over the available buckets.  The buckets
of the grown table have an average fill degree of less than 50% while
the other tables have a more even element distribution.  This effect
is only accentuated by the effects of inhomogeneous table resolution.
Since the newly grown table also has less associated elements per
bucket than the smaller tables.

Assume the following, all tables are filled completely when the first
table grows. Now there is new capacity for elements, but only in
the first second level table. So when we insert an element, we have to
find a way to move a previous element into the first table. When
inserting more and more elements, there are less and less elements
that can be moved into the first table.  This problem is intensified
by the observation made in (TODO previous thing about inhomogeneous
...). To fill the first table one would have to store about
$2n/(/tl/+1)$ elements there, which is hard because only about $2n//tl/$
of elements that is associated to buckets in that table.

# less severe if elements are deleted from the table  (grows slower) approaches random distribution

*** Population Density
Since our table grows with the number of elements, it is always
densely filled.  Thus, insertions naturally take longer than they
would if they were performed on an emptier table.

# This is kind of the point right? otherwise can still be initialized to be larger.

** Evaluating less Hash Functions
Evaluating a good hash function can take multiple CPU cycles, that can
usually not be overlaid with different tasks, because, their result is
important for the remaining operation (e.g. lookup find associated
buckets). Therefore, reducing the number of hash function computations
can save a lot of time.

To understand the following technique, we think about the potential
size of a hash table, and its number of buckets. Using $32$ bits one can
address $2^32$ buckets ($\approx 4\,$G) -- with a bucket size of eight
this makes $2^35\approx 34\,$G.  With a common element size of $64\,$bit per
key and $64\,$bit per value -- $16\,$Byte per pair -- the table has a size of
$512\,$GiB. This is big enough for any reasonable /single threaded/
application.  Therefore, instead of computing two hash functions, we
use one $64\,$bit hash function and split the result, to compute both
associated buckets.  For this to work, one has to use a hash function
which generates $64\,$bits of "randomness".

* Experiments
** Comparison Implementations
** Hardware
** Tests
*** Incremental Construction
*** Mixed Benchmarks
