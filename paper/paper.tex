% -*- mode: latex; mode:writegood mode: flyspell; ispell-local-dictionary: "en_US"; coding: utf-8 -*-

\documentclass[a4paper,UKenglish]{lipics-v2016}
%This is a template for producing LIPIcs articles.
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
% for section-numbered lemmas etc., use "numberwithinsect"

\usepackage{microtype}%if unwanted, comment out or use option "draft"

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the recommended bibstyle

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{DySECT -- truly space efficient dynamic hashing} % -- a novel approach to growing!}
\titlerunning{DySECT -- truly space efficient dynamic hashing} %optional, in case that the title is too long; the running title should fit into the top page column

%% Please provide for each author the \author and \affil macro, even when authors have the same affiliation, i.e. for each author there needs to be the  \author and \affil macros
\author[1]{Tobias Maier}
\author[1]{Peter Sanders}
\affil[1]{Karlsruhe Institute of Technology, Karlsruhe, Deutschland\\
  \texttt{\{t.maier, sanders\}@kit.edu}}
%% \affil[2]{Department of Informatics, Dummy College, Address/City, Country\\
%%   \texttt{access@dummycollege.org}}
\authorrunning{T. Maier and P. Sanders} %mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Tobias Maier and Peter Sanders}%mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{Dummy classification -- please refer to \url{http://www.acm.org/about/class/ccs98-html}}% mandatory: Please choose ACM 1998 classifications from http://www.acm.org/about/class/ccs98-html . E.g., cite as "F.1.1 Models of Computation".
\keywords{Dynamic data-structures, Open addressing, Closed hashing, Cuckoo hashing, Dense hash tables}% mandatory: Please provide 1-5 keywords
% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Acces}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{hyperref}
%\usepackage{graphix}



\begin{document}

\maketitle

\begin{abstract}
Research has concentrated on optimizing the speed and space efficiency
of hash tables for a long time.  Hash tables using modern open
addressing techniques can be filled to more than 95\% before the
performance deteriorates significantly.  Once this threshold is met,
copying and migrating the whole table still seems to be the most
efficient answer.  Growing by a constant factor $\gamma$ often leads
to bad space efficiency because the filling degree of the grown table
is at most $\gamma^{-1}$. During the migration, the space efficiency
is even worse, since oftentimes both the new and the old table coexist
at the same time, causing a massive space overhead.

We propose a new hash table
architecture that supports dynamic growing while keeping a strict size
constraint $\alpha \cdot n$ throughout its lifetime.  Even under these
conditions, we achieve guaranteed constant lookup times, and amortized
constant insertion times.

Our architecture splits the overall hash table into subtables that
grow independent from each other.  The cuckoo load balancing mechanism
can move elements between subtables ensuring that growing one
subtable increases the overall insertion opportunities.
 \end{abstract}

%% \section{Typesetting instructions -- please read carefully}
%% Please comply with the following instructions when preparing your article for a LIPIcs proceedings volume.
%% \begin{itemize}
%% \item Use pdflatex and an up-to-date LaTeX system.
%% \item Use further LaTeX packages only if required. Avoid usage of packages like \verb+enumitem+, \verb+enumerate+, \verb+cleverref+. Keep it simple, i.e. use as few additional packages as possible.
%% \item Add custom made macros carefully and only those which are needed in the article (i.e., do not simply add your convolute of macros collected over the years).
%% \item Do not use a different main font. For example, the usage of the \verb+times+-package is forbidden.
%% \item Provide full author names (especially with regard to the first name) in the \verb+\author+ macro and in the \verb+\Copyright+ macro.
%% \item Fill out the \verb+\subjclass+ and \verb+\keywords+ macros. For the \verb+\subjclass+, please refer to the ACM classification at \url{http://www.acm.org/about/class/ccs98-html}.
%% \item Take care of suitable linebreaks and pagebreaks. No overfull \verb+\hboxes+ should occur in the warnings log.
%% \item Provide suitable graphics of at least 300dpi (preferrably in pdf format).
%% \item Use the provided sectioning macros: \verb+\section+, \verb+\subsection+, \verb+\subsection*+, \verb+\paragraph+, \verb+\subparagraph*+, ... ``Self-made'' sectioning commands (for example, \verb+\noindent{\bf My+ \verb+subparagraph.}+ will be removed and replaced by standard LIPIcs style sectioning commands.
%% \item Do not alter the spacing of the  \verb+lipics-v2016.cls+ style file. Such modifications will be removed.
%% \item Do not use conditional structures to include/exclude content. Instead, please provide only the content that should be published -- in one file -- and nothing else.
%% \item Remove all comments, especially avoid commenting large text blocks and using \verb+\iffalse+ $\ldots$ \verb+\fi+ constructions.
%% \item Keep the standard style (\verb+plainurl+) for the bibliography as provided by the\linebreak \verb+lipics-v2016.cls+ style file.
%% \item Use BibTex and provide exactly one BibTex file for your article. The BibTex file should contain only entries that are referenced in the article. Please make sure that there are no errors and warnings with the referenced BibTex entries.
%% \item Use a spellchecker to get rid of typos.
%% \item A manual for the LIPIcs style is available at \url{http://drops.dagstuhl.de/styles/lipics-v2016/lipics-v2016-authors/lipics-v2016-manual.pdf}.
%% \end{itemize}

\section{Introduction}
Hash tables are some of the most frequently used data-structures. Having
interchangeable implementations which perform well under different
conditions is important and allows programmers to easily adapt known
solutions to new circumstances.

One aspect a lot of research focuses on is space efficiency TODO
REFERENCES.  Modern space efficient hash tables work well up to loads
of 95\% and more.  But for this to matter, the table has to be
initialized with the correct final capacity, therefore, forcing
programmers to know tight bounds to the maximum number of inserted
elements.  This is typically not the case.  One of the most ubiquitous
use cases for hash tables is to unify data elements by their key.
Whenever the exact number of unique keys is not known a priori we have
to overestimate the initial capacity to guarantee good performance.
Dynamic space efficient data-structures are necessary to alleviate
this problem independent of the circumstances.

To visualize this assume the following scenario.  During the operation
of a word count benchmark, we know an upper bound $c$ to the number of
unique words. Therefore, we construct a table large enough to hold
these elements ($0.9^{-1}\cdot c$ cells).  It is entirely possible
that the specific word count instance only contains $0.7\cdot c$
unique words.  The resulting table is therefore only filled to 63\%.
It is clear that better static sized tables cannot achieve fill ratios
greater than 70\%.  A factor of 0.7 seems realistic considering
that the benchmark from Section~\ref{sec:exp_wordcount} for example
contains 240\,000\,000 words only 20\,000\,000 of which are unique.

Depending on the underlying problem it is impossible to tightly
estimate the final number of elements. Since growing the table
dynamically is independent from the problem domain, it can be a
solution even in these instances.  To achieve space efficiency in
scenarios where the final size is not known the hash table has to grow
closely with the actual number of elements.  This cannot be achieved
efficiently Swith any of the current techniques used for hashing and
migration.

Many libraries -- even ones that implement space efficient hash tables
-- already offer growing.  The problem with these implementations is
that they either lose their space efficiency or their performance
degrades once the table grows above its original capacity.  Usually
growing is implemented by either creating additional hash tables --
decreasing the performance especially for look ups, or by migrating
all elements to a new table -- losing the space efficiency by
multiplying the original size.

To avoid both of these pitfalls we propose a variant of bucket cuckoo
hashing (TODO REFERENCE).  Bucket cuckoo hashing is a technique where
each element can be stored in one of several associated constant sized
buckets.  When all of them are full we move an element into another of
its buckets to create new space.  To solve the problem of efficient
migration, we split the table into multiple subtables each of which
can grow independently from all others.  Because the buckets
associated with one element are spread over the different subtables,
growing one subtable alleviates pressure from all others by allowing
moves from a dense subtable to the grown subtable.

Doubling the size of one subtable increases the overall size by a
small factor, while only moving a small number of elements -- making
the size changes easy to amortize. The introduced size and occupancy
imbalance between subtables is alleviated using displacement
techniques common to cuckoo hashing. Using these techniques, makes our
table work and grow efficiently with fill degrees of 95\% and more.
Being approximately TODO \ldots{} \% faster than common hash table
techniques under the same circumstances.

%% \section{Motivation}
%% \label{sec:motivation}
%% Space efficiency is a major factor for the development of new hashing
%% techniques.  The goal has long been to make hash tables work better in
%% high load scenarios.  Modern hashing techniques like Hopscotch, Robin
%% Hood, and Cuckoo Hashing can easily fill hash tables to more than 90\%
%% and up to 98\% while still achieving constant average running times on
%% most of their operations.  But since these techniques depend on
%% statically sized tables this only makes sense, if the number of unique
%% elements can be estimated with similarly tight bounds.

%% To visualize this assume the following scenario.  During the operation
%% of a word count benchmark, we know an upper bound $c$ to the number of
%% unique words. Therefore, we construct a table large enough to hold
%% these elements ($0.9^{-1}\cdot c$ cells).  It is entirely possible
%% that the specific word count instance only contains $0.7\cdot c$
%% unique words (or less).  The resulting table is therefore only filled
%% 63\%.  It is clear that better static sized tables cannot achieve fill
%% ratios greater than 70\%.

%% Depending on the underlying problem it is impossible to tightly
%% estimate the final number of elements. Since dynamic growing is
%% independent from the problem's domain, it can be a solution even in
%% these instances.  To achieve space efficiency in scenarios where the
%% final size is not known the hash table has to grow closely with the
%% actual number of elements.  This is not efficient with any of the
%% current techniques used for hashing and migration.

\section{Related Work}
The usage of hash tables, and other hashing based algorithms has a
long history in computer science.  The classical methods and results
can be found in all major algorithm textbooks (TODO ...).

Over the last one and a half decades, the field has regained
attention, both from theoretical and the practical point of view.  The
initial innovation that sparked this attention was the idea, that
storing an element in the less filled of two ``random'' chains leads to
incredibly well balanced loads.  This concept is called the power of
two choices~\cite{ThePowerOfTwoChoicesInRandomizedLoadBalancing}.

It led to the development of cuckoo hashing~\cite{CuckooHashing}.
Cuckoo hashing extends the power of two choices by allowing to move
elements within the table when all chosen positions are already
occupied (see Section~\ref{sec:pre_cuckoo} for a more elaborated
explanation).  Cuckoo hashing revitalized research into space
efficient hash tables.  Probabilistic bounds for the maximum fill
degree and expected displacement distances are often highly
non-trivial TODO SOME BOUNDS PAPERS.

Cuckoo hashing also spawned many variants that further improve its
performance.  The most notable one being bucket cuckoo hashing
($d$-ary cuckoo hashing) a variant that groups cells together in
buckets thus improving cache usage, increasing displacement
opportunities and maximum fill degree TODO BUNCH OF PAPERS.  There are
even further variations on bucket cuckoo hashing, like the
generalization to continuous buckets TODO PAPER.

Further adaptations of cuckoo hashing include:multiple concurrent implementations either powered by bucket
locking, transactional
memory~\cite{AlgorithmicImprovementsForFastConcurrentCuckooHashing},
or fully lock-less~\cite{LockFreeCuckooHashing}; and a de-amortization
technique that allows a provable worst case analysis of
insertions~\cite{UsingAQueueToDeAmortizeCuckooHashingInHardware,
  DeAmortizedCuckooHashingProvableWorstCasePerformanceAndExperimentalResults}.

Some non-cuckoo space efficient hash tables continue to use linear
probing variants.  \emph{Robin Hood hashing} is a technique that was
originally introduced in 1985~\cite{RobinHoodHashing}. The idea behind
Robin Hood hashing is to move already stored elements during
insertions in a way that minimizes the longest possible search
distance.  Robin Hood hashing has regained some popularity in recent
years -- mainly for its interesting theoretical properties and the
possibility to reduce the inherent variance of linear probing.
\emph{Hopscotch hashing}~\cite{HopscotchHashing} is a technique, that
originated in the research of concurrent hash tables, but it can also
manage densely filled hash tables.  Hopscotch hashing also moves
elements within the table to reduce the maximum search distance.
Additionally it introduces an acceleration data structure that is
designed to improve lookup performance especially in cases where the
searched key is not found.

All these publications show that there is a clear interest in
developing hash tables that can be more and more densely filled.
Dynamic hash tables on the other hand seem to be considered a solved
problem.  One paper that takes on the problem of dynamic hash tables
was written by Dietzfelbinger at
al.~\cite{DynamicPerfectHashingUpperAndLowerBounds}.  It actually
predates cuckoo hashing, and much of the attention for space efficient
hashing.  All memory bounds presented are given without tight constant
factors.  The lack of implementations and theory about dense dynamic
hash tables is where we pick up and offer a fast hash table
implementation, that supports dynamic growing with tight space bounds.

\section{Preliminaries}
A hash table is a data-structure for storing key-value-pairs
($\langle key, data \rangle$) and offers the following functionality:
\verb~insert~ -- stores a given key-value pair; \verb~find~ -- given a
key returns if said key was stored, and $\bot$ otherwise; and
\verb~erase~ -- deletes a previously inserted element (if present).

An alternative model which is sometimes used in literature considers
arbitrary elements instead of key value pairs.  In that model, the key
is extracted using an extractor function.  All techniques presented in
this paper also work in the extractor model without any adaptation.

Throughout this paper $n$ denotes the number of elements and $m$ the
number of cells ($m > n$) in any given hash table.  We define the load
factor as $load = n/m$ and the fraction of empty cells $\varepsilon =
1-load$.  Tables can usually only work efficiently up to a certain
load factor.  Above that, operations get slower and/or have a
possibility to fail.  When implementing a hash table one has to decide
between storing elements directly in the hash table -- \emph{Closed
  Hashing} -- or storing pointers to elements -- \emph{Open
  Hashing}. This has an immediate impact on the necessary memory:
\begin{align*}
  mem_{closed}(n,m) &\geq m\cdot size(element)\\
  mem_{open}  (n,m) &\geq m\cdot size(pointer) + n\cdot size(element)\\
\end{align*}

For large elements ($\gg size(pointer)$), one can use a non-space
efficient hash table with open hashing to reduce the relevant memory
factor.  Therefore, we restrict ourselves to the common and more interesting case of
elements whose size is comparable to that of a pointer.  For our
experiments we use 128bit elements (64bit keys and 64bit values).  In
this case, open hashing introduces a significant memory
overhead (at least $\times1.5$). For this reason, we only consider closed
hash tables. For them, the
memory efficiency is directly dependent on their load.  To
reach high fill degrees with closed hashing tables, we have to employ
\emph{open addressing} techniques which means that elements are not
stored in predetermined cells, but can be stored in one of several
options (e.g. linear probing, or cuckoo hashing).

\subsection{$\alpha$-Space Efficient Hash Tables}
\subparagraph*{Static}
\label{sec:pre_staticspace}
We call a hashing technique $\alpha$-space efficient when it can work
effectively using at most $\alpha \cdot n_{\textit{curr}} \cdot \textit{size}(\textit{element}) + O(1)$ memory. In
this case we define working efficiently as having average insertion
times in $O(\frac{1}{\varepsilon})$. This is a natural bound
since it is the expected number of fully random probes needed to hit
an empty cell.

%\[\alpha \cdot size_min = \alpha\cdot n\cdot size(element)\]

In many closed hashing techniques (e.g.~linear probing, cuckoo
hashing) cells are the same size as elements. Therefore, being
$\alpha$-space efficient is the same as operating with a load factor
of $\alpha^{-1} = n/m$.
%(because $\alpha\cdot n \cdot size(element) = m\cdot size(element)$).
Because of this, we will mostly talk about the load factor of a table
instead of its memory usage.

Some techniques like hopscotch hashing use additional per cell
information.  These hash table architectures, have to be filled more
densely to achieve the same space efficiency.

\paragraph*{Dynamic}
The definition of a space efficient hashing technique given above is
specifically targeted for statically sized hash tables.  We call an
implementation dynamically $\alpha$-space efficient if an instantiated
table can grow arbitrarily large over its original capacity, while
remaining smaller than $\alpha\cdot n_{\max}\cdot
\textit{size}(\textit{element}) + O(1)$ at all times.
%\[mem_{curr} \leq \alpha\cdot max(n_{curr})\cdot size(element)\]

One problem for many implementations of space efficient hash tables is
the migration.  During a normal full table migration, both the
original table, and the new table are allocated.  This takes
$\textit{m}_{\textit{new}} + \textit{m}_{\textit{old}}$
cells. Therefore, a normal full table migration is never better than 2-space
efficient.  The only option to perform a full table migration with
less memory is to increase the memory in place (see
Section~\ref{sec:dys_inplace} and Section~\ref{sec:exp_competitor}).

Similar to static $\alpha$-space efficiency, we will mostly talk about
the \emph{minimum load factor} instead of $\alpha$.  We note that this
definition of dynamic $\alpha$-space efficient hash tables does not
enforce size reduction due to the deletion of elements. However it
enforces that cells previously used by deleted elements must be
\emph{reused}.

\subsection{Cuckoo Hashing}
\label{sec:pre_cuckoo}
Cuckoo hashing is a technique to resolve hash conflicts in a closed
hashing hash table. Its main draw is that it guarantees constant
lookup times even in densely filled tables.  The distinguishing
technique of cuckoo hashing is that $H$ hash functions ($h_1, ... ,
h_H$) are used to compute $H$ independent positions. Each element is
stored in one of its positions.  Even if all positions are occupied
one can often move elements to create space for the current
element. We call this process \emph{displacing} elements.

TODO MAYBE ALREADY GRAPH VIEW

Bucket cuckoo hashing is a variant, where the cells of the hash table
are grouped into buckets of size $B$ ($m/B$ buckets).  Each element
assigned to one bucket can be stored in any of the bucket's cells .
Using buckets one can drastically increase the number of elements that
can be displaced to make room for a new one, thus decreasing the
expected length of displacement paths.

\emph{Find} and \emph{remove} operations have a guaranteed constant
running time. Independent from the tables density there are $H$
buckets -- $H\cdot B$ cells -- that have to be searched to find an
element.

During an \emph{insert} the element is hashed to $H$ buckets.  If at least
one of those buckets has an empty cell, we store the element in the
bucket with the most free space.  When all buckets are full we
have to move elements within the table such that a free cell becomes
available.

\label{sec:cuckoo_graph}
To visualize the problem of displacing elements, one can think of the
directed graph implicitly defined by the hash table.  Each bucket
corresponds to a node and each element induces an edge between
the bucket it is stored in and its $H-1$ alternate buckets.  To insert
an element into the hash table we have to find a path from one of its
associated buckets, to a bucket, that has space remaining.  Then we
move elements along this path to make room in the initial bucket.
The two common techniques to find such paths are \emph{random walks} and
\emph{breadth first searches}.

\section{DySECT (Dynamic Space Efficient Cuckoo Table)}
A commonly used growing technique is to double the size of a hash
table by migrating all its elements into a table with twice its
capacity.  This is of course not memory efficient.  The idea behind
our hash table architecture is to double only parts of the overall
data-structure.  This increases the space in part of our
data-structure without changing the rest.  We then use cuckoo
displacement techniques to make this additional memory reachable from
other parts of the hash table.

\subsection{Overview}
Our DySECT hash table consists of $T$ subtables (shown in TODO
FIGREFRENCE), that in turn consist of buckets that store elements.
Each element has $H$ associated
buckets similar to cuckoo hashing which can be in the same or
in different subtables.  To find a bucket associated with an element $e$,
we compute $e$'s hash value using the appropriate hash function
$h_i(e)$. The hash is then used to compute the subtable and the bucket
within that subtable.

To make this efficient we use powers of two for the number of
subtables ($T = 2^t$), as well as for the individual subtable sizes ($s =
2^x*B$).  Since the number of subtables is constant we can use the
first $t$ bits from the hashed key, to find the appropriate subtable.
From the remaining bits we compute the bucket within the subtable using
a bitmask ($h_i(e)\,\texttt{\&}\,(2^x-1) = h_i(e) \mod 2^x$).

%% We split that hash value into two parts, then we use the
%% first part to chose a subtable, and the second part to
%% choose one of the contained buckets.

%% To understand the performance of this hash table, it is important to
%% realize that when the hash table is used regularly, the first level
%% table will always remain cached. This is important, because otherwise lookups
%% would cause unnecessary cache misses that would not have happened in a
%% single level hash table.  Because of this we expect the average number
%% of cache misses during hash table operations to be similar to those of
%% a normal cuckoo hash table.

\subsection{Growing}
We grow one subtable by migrating it into a table twice its size
as soon as the (overall) table contains enough elements such that the memory constraint
can be kept during the migration.  We migrate subtables in order
from the first to the last. This ensures that no subtable can be more than
twice as large as any other.

Assume that all subtables have the same size ($s=m/T$). When $\alpha\cdot n >
m+2s$ we can grow the first subtable while keeping the size
constraint.  Doubling the size of a subtable increases the global
number of cells from $m_{old} = T\cdot s$ to $m_{new} = m_{old}+s =
(T+1)\cdot s$ (grow factor $\frac{T+1}{T}$).  Note that all subsequent
growing operations migrate one of the smaller tables, until all tables have the
same size again.  Therefore, each grow until then increases the
overall capacity by the same absolute amount (smaller overall factor TODO EXPLAIN FURTHER?).

The cost of growing a subtable is amortized by all insertions that
happened since the last subtable migration.  There are at least
$\alpha^{-1} \cdot s = \Omega(s)$ insertions between two migrations.
One migration takes $\Theta(s)$ time.  Apart from being amortized, the
migration is cache efficient since it accesses cells in a linear
fashion.  Even in the target table cells are accessed linearly.  We
assign elements to buckets by using bits from their hash value. In the
grown table we use exactly one more bit than before (double the number
of buckets).  This ensures, that all elements from one original
bucket, are split between two buckets in the target table.
Therefore, no bucket can overflow and no displacements are necessary.

In the implicit graph model of the cuckoo table
(Section~\ref{sec:cuckoo_graph}), growing a subtable is equivalent to
splitting each node that represents a bucket within that table. Since
the edges (elements) in the implicitly defined subgraph are not
doubled, the resulting subgraph becomes more sparse, making it easier
to insert elements.

\subsection{Shrinking}
In many use cases automatic shrinking is not necessary.  It worsens
performance by taking time for the migration and making the remaining
table more dense.  Therefore, we forgo automatic shrink operations
whenever enough elements are removed from the table.

If shrinking is necessary it can work similarly to growing. We
replace a subtable with a smaller one by migrating elements from one
to the other.  During this migration we join elements from two buckets
into one. Therefore, it is possible for a bucket to overfill.  We
reinsert these elements at the end of the migration.  Obviously, this
can only apply to at most half the migrated elements.

When automatically triggering the size reduction, one has to make sure
that the migration cost is amortized. Therefore, a grow operation cannot
immediately follow a shrink operation.  When shrinking is enabled we
propose to shrink one subtable when $\alpha\cdot n < m-s'$ elements
($s'$ size of a large table, $m_{new} = m_{old} -s'/2$).
Alternatively, one could implement a \emph{shrink to size} operation that is
explicitly called by the user.

Notice that the allocated memory actually increases during the
migration itself.

\subsection{Difficulties for the Analysis of our table structure}
There are two factors that impact the performance of our dynamic
table compared to other cuckoo table variants and to other hashing
solutions in general: \textbf{inhomogeneous table resolution} and
\textbf{element imbalance}.
%, and \textbf{population density}.
Both factors influence the maximum load density and the
running times in different ways.

\subparagraph*{Imbalance through Inhomogeneous Table Resolution.}
\label{sec:inhom_res}
By growing subtables individually we introduce a size imbalance
between subtables. Large subtables contain more buckets but the number
of elements hashed to a large subtables is not generally higher than
the number of elements that are hashed to a small subtable.  This
makes it difficult to spread elements evenly among buckets.
Imbalanced bucket fill ratios can lead to longer insertion times.

Assume there are $n$ elements in a hash table with $T$ subtables,
$j$ of which have size $2s$ the others have size $s$. If elements are
spread equally among buckets then all small tables have around
$n/(T+j)$ elements, and the bigger tables have $2n/(T+j)$ elements.
For each table there are about $Hn/T$ elements associated to one of
its buckets.  This shows that having more hash functions can lead to a
better balance.

For two hash functions ($H=2$) and only one grown table ($j=1$) this
means that $\approx 2n/(T+1)$ elements should be stored in the first table to
achieve a balanced bucket distribution.  Therefore, nearly all
elements associated with a bucket in the first table ($\approx 2n/T$)
have to be stored there.  This is the reason why $H=3$ leads to
significantly better space efficiency than $H=2$.

\subparagraph*{Imbalance through Size Changes.}
In addition to the problem of inhomogeneous tables there is an
inherent balancing problem introduced by resizing subtables. It is
clear that a newly grown table is not filled as densely as other
tables.  Since we double the table size, grown tables can only be
filled to about 50\%.

Assume the global table is filled close to 100\% when the first table
begins to grow.  Now there is capacity for $s$ new elements but this
capacity is only in the first table, elements that are not hashed to
the first table, automatically trigger displacements leading to slow
insertions.  Notice that repeated insert and erase operations help to
equalize this imbalance, because elements are more likely inserted
into the sparser areas, and more likely to be deleted from denser
areas.

% \subparagraph*{Population Density}
% Measuring and comparing the performance of space efficient growing
% tables to their statically sized counterparts is difficult.  Inserting
% elements into a densely filled table takes longer, than into an empty
% one.  Therefore, a table that is always densely filled will naturally
% be slower than one that only fills up towards the end of the
% execution -- even when ignoring eventual growing costs.

\subsection{Implementation Details}
For our experiments in Section~\ref{sec:exp} we use three hash
functions ($H=3$) and a bucket size of ($B=8$). These values have
consistently outperformed other options.  $T$ is set to 256
subtables for all our tests.  The number of subtables only plays a
minor role for the running time.  To find displacement opportunities
we use breadth first search.  In our tests it performed better than
random walks, since it better uses the read cache lines from one
bucket.

\subparagraph*{Reducing Number Computed Hash Functions}
Evaluating hash functions is expensive, therefore, reducing the number
of hash functions computed per operation increases the performance of
the table.  The hash function we use computes 64bit hash values
(i.e. xxHash TODO).  We split the 64bit value into two 32bit hash
values. All common bucket hash table sizes can be addressed using 32
bits. They can address up to $2^{32}$ buckets which achieves a
capacity for $2^{35} \approx 34$ billion elements (bucket size 8),
at that point a hash table consumes 512GiB memory.

When $H > 2$ we can use \emph{double hashing} TODO CITE to further reduce the number
of computed hash functions. Double hashing creates an arbitrary number
of hash values using only two original hash functions $h'$ and
$h''$.  The additional values are linear combinations computed
from the original two values, $h_i(key) = h'(key) + i\cdot
h''(key)$.

Combining both of these techniques, we can reduce the number of
computed hash functions to one 64bit hash function.  This is
especially important during large displacements, where each
encountered element has to be rehashed to find its alternative
buckets.

\subparagraph*{Table Architecture (two Variants)}
\label{sec:dys_inplace}
The hash table can be implemented as a collection of pointers to
subtables. We have to lookup the corresponding pointer whenever a
subtable is accessed.  This does not impact performance much since all
subtable pointers will be cached -- if the hash table is a performance bottleneck.

Another option for implementing subtables is to use virtual memory,
and memory overcommiting.  The idea is that the operating system will
usually allow larger memory allocations than the machine has memory,
with the anticipation that not all the allocated memory will actually
be used.  Only memory pages, that are actually used will be mapped
from virtual to physical memory pages.  Thus from a strict resource
point of view, the memory is not yet used.  Therefore, we can also
implement the subtables as sections within one huge allocation.  This
has the advantage that the offset for each table can be computed
quickly, without looking it up from a table.

The added advantage is that we can grow subtables in-place.  To
increase the size of a subtable, it is enough to initialize a
consecutive section of the table (following the original subtable).
Once this is done, we have to redistribute the table's elements.  This
allows us to grow a subtable without the space overhead of
reallocation.

\section{Experiments}
\label{sec:exp}
There are many factors that impact hash table performance.  Many of
these factors are inherent to the combination of growing and space
efficiency.  To show that our ideas work in practice we use both
micro-benchmarks and practical experiments.

All reported numbers are averaged by running each experiment five
times.  All measurements, where more than 100 operations failed are
omitted, this happens mostly for hopscotch hashing.  The experiments
were executed on multiple machines.
TODO...

The results were very similar, therefore, we only report the numbers
achieved on the

TODO
...


\subsection*{Competitors}
\label{sec:exp_competitors}
To put the performance of our solution into perspective, we
implement and test several other options for space efficient
hashing.  We use our own implementations, since no hash table found
online supports strict space-efficiency.

\subparagraph*{Linear Probing} is the most common closed hashing
technique.  It is knownaccepted to be inefficient for higher load
factors because search distances can become large.  In practice, this
fact is counteracted by its cache-efficiency making operations fast in
spite of long probing distances.  Linear probing is especially slow in
\verb~find~ and \verb~erase~ operations which often have guaranteed
constant running time in other implementation.

\subparagraph*{Hopscotch Hashing} is a technique similar to linear probing where
elements are moved when they are too far from their original hash
position.  Additionally, hopscotch hashing introduces an acceleration
data structure which uses bitmaps to store neighborhood data. This
data improves \verb~find~ and \verb~erase~ performance.  The
bitmaps introduce a significant memory overhead of $|$neighborhood$|$
bits per cell.  Our experiments show that for any reasonable fill
factor neighborhoods have to be 64bits and larger.  This introduces a
size factor of $\times1.5$ (128bit elements).  We still include
measurements with hopscotch hashing \emph{without considering space for
the neighborhood data} to show its overall performance tendencies.

Since 64bit neighborhoods failed on many of our tests, we implemented
a variant with neighborhoods of 128 cells.  In this variant, we only
store one bitmask per two cells (each bit of the bitmask denotes two
cells). One could call this variant bucket Hopscotch hashing.

\subparagraph*{Robin Hood Hashing} is a linear probing variant that
minimizes the maximum search distance.  The drawback of this technique
is that insertions have to compute the hash of each element that is
skipped by the insertion.

\subparagraph*{Classic Bucket Cuckoo Hashing} as described in
Section~\ref{sec:pre_cuckoo}, similar to DySECT we choose $H=3$ and
$B=8$ since they have the optimal performance.

\subparagraph*{Growing our Competitors}
To grow these competitor implementations, we use in place table
migrations.  This is possible due to over allocation (similar to
Section~\ref{sec:dys_inplace}).  We allocate a large piece of memory,
when the table is created but only a small fraction of this memory is
initialized.  Increasing the memory of the hash table is similar to
initializing a new section of the allocated memory.  In addition to
this allocation technique we implement some simple in place migration
algorithms (APPENDIX OR SECTION?).  These algorithms are relatively
cache efficient, and reach a performance similar to other migrations
(by reinsertion of elements).

For a table that was initialized with a min load factor
$1-\varepsilon$ we trigger growing once the table is loaded more than
$1-\frac{\varepsilon}{2}$. We then increase the capacity $m$ to
$(1-\varepsilon)^{-1}\cdot n$.  Our implementation of the migration is
highly cache efficient, but all elements have to be moved.  This makes
repeated migrations inefficient especially for small $\varepsilon$.

\subsection{Influence of Fill Ratio (static table size)}
\label{sec:exp_eps}

The following test was performed by initializing a table with
$m\approx25\,000\,000$~cells (non-growing). Then elements are inserted
until there is a critical amount of failing insertions ($>100$).  At
different stages, we measure the running time of new insertion
(Figure~\ref{fig:eps_insert}), and find (Figure~\ref{fig:eps_find})
operations (averaged over 1000 operations).  Finds are measured using
either randomly selected elements from within the table (successful),
or by searching random elements from key space (unsuccessful).

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/eps_insert.pdf}
  \caption{\label{fig:eps_insert}Insert timings at different fill
    degrees into a non-growing table.}
\end{figure}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\textwidth]{../eval/plots/eps_find_s.pdf}
  \includegraphics[width=0.49\textwidth]{../eval/plots/eps_find_u.pdf}
  \caption{Timings for find operations with different fill degrees
    (\emph{left:}successful, \emph{right:}unsuccessful).}
  \label{fig:eps_find}
\end{figure}

As one might expect, the performance of insert operations depends
highly on the fill degree of the table.  Therefore, we show it
normalized with $\frac{1}{\varepsilon}$, which is the expected number
of fully random probes to find a free cell and thus a natural estimate
for the running time. We see that -- up to a certain point -- the
insertion time behaves proportional to $\frac{1}{\varepsilon}$ for all
tables, then it begins to decrease uncontrolled.  The running time of
our table remains stable for longer than linear probing variants, but
not as long, as for classic cuckoo hashing.  Hopscotch hashing fails
earlier than all other tables, because its bitmaps become to small to
store the neighborhood information.

DySECT's performance degrades earlier than that of cuckoo hashing.  One reason
for this is that our hash table consists of $T$ subtables with power
of two sizes and thus does not allow exactly 25\,000\,000 cells.  Therefore,
our table is slightly more full.  Another reason is that in order to get close
to 25\,000\,000 cells, we have 125 large subtables (twice the size of
the others).  This introduces inhomogeneous table resolution (see
Section~\ref{sec:inhom_res}), which decreases our performance compared to
cuckoo hashing.

Figure~\ref{fig:eps_find} shows that linear probing variants perform
badly for find operations on highly filled tables.  It also shows that
unsuccessful find operations are often very slow when using linear
probing.  This is where the acceleration data-structure used by
hopscotch tables can significantly improve performance.  Cuckoo hashing variants like DySECT have
guaranteed constant running times for all find operations --
independent of their success and the table's filling degree.

\subsection{Influence of Fill Ratio (dynamic table size)}
\label{sec:exp_ti}
%% \begin{figure}
%%   \includegraphics[width=0.48\textwidth]{../eval/plots/ti_find_s.pdf}
%%   \includegraphics[width=0.48\textwidth]{../eval/plots/ti_find_u.pdf}
%%   \caption{Timings for find operations on the grown table (from Figure~\ref{fig:ti_insert}; \emph{left:}successful, \emph{right:}unsuccessful).}
%%   \label{fig:ti_find}
%% \end{figure}
In this test 20\,000\,000 elements are inserted into an initially empty
table.  The table is initialized expecting 50\,000 elements, thus
growing is necessary to fit all elements.  The tables are configured
to guarantee a load factor of at least
$1-\varepsilon$. Figure~\ref{fig:ti_insert} shows the performance in
relation to the load factor.  Insertion times are computed as average of all 20\,000\,000 insertions. They are normalized similar to
Figure~\ref{fig:eps_insert} (divided by $\frac{1}{\varepsilon}$).

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/ti_insert.pdf}
  \caption{\label{fig:ti_insert}Average time per insert operation
    during the construction of a dynamic table.}
\end{figure}

We see that DySECT performs by far the best even with less filled
tables.  Here we achieve a speedup of TODO$\%$ over the next best
solution (blabla vs. linear probing blabla at $\varepsilon = 0.$ bla).
On denser instances, we can increase this margin to TODO$\%$ (blabla
vs we'll see blabla at $\varepsilon = 0.$ bla).  With shrinking
$\varepsilon$, we see that insertion times start to degrade for our
competitors.  DySECT remains close to $O(\frac{1}{\varepsilon})$ even
for fill degrees up to 97.5$\%$ (TODO check).

We also measured the performance of find operations on the created
tables, they are similar to the performance on the static table in
Section~\ref{sec:exp_eps} (see Figure~\ref{fig:eps_find}), therefore,
we omit displaying them for space reasons.

\subsection{Word Count a Practical use Case}
One of the most common use cases for hash tables -- dynamic hash
tables in particular -- is to unify data elements according to their
key.  In these cases, only dynamic hash tables can remain space
efficient because the number of unique keys is often unknown.  In
the following test, we take the first block of the common crawl
data-set (TODO REFERENCE) and compute a word count of the contained
words.  The chosen block contains around 240\,000\,000 words, with around 20\,000\,000
unique words. For the test, we hash each word and insert it's hash
together with a counter.  Subsequent insertions of a re-occurring word
increase this counter.  Similar to the growing benchmark, we start
with an empty table initialized for 50\,000 elements.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/crawl.pdf}
  \caption{\label{fig:crawl} }
\end{figure}

The performance results can be seen in Figure~\ref{fig:crawl}.  We do
not use any normalization since each word is repeated 12 times (on
average).  Therefore, most inserts will actually behave like
successful find operations.  Our DySECT table manages to dominate the
field, similar to the growing test in Section~\ref{sec:exp_ti}.  This
indicates that insert performance can dominate running times even in
find intensive workloads.  This insight is intensified in the
following test where we mix \verb~insert~ and \verb~find~ operations.
(Section~\ref{sec:exp_mix}).


\subsection{Access Patterns/Combining operations}
\label{sec:exp_mix}
In this test, we show how the hash tables behave under mixed
workloads.  We use tables with a minimum load factor of $\varepsilon =
0.95$ (TODO verify).  The test starts a decently filled table
(15\,000\,000 elements).  On this table we perform 10\,000\,000
operations mixed between \verb~insert~ and \verb~find~/\verb~erase~
operations.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\textwidth]{../eval/plots/mix.pdf}
  \includegraphics[width=0.49\textwidth]{../eval/plots/mixd.pdf}
  \caption{\label{fig:mix} }
\end{figure}

This test clearly shows that the
performance of insertions is very important even if there are
significantly more finds than insertions.  This is even more
accentuated by the fact that all performed finds are successful finds,
which have usually significantly better performance than unsuccessful
ones (especially for linear probing; see Figure~\ref{fig:eps_find}).

\subsection{Conclusion}
Our Experiments have shown that our DySECT hash table is by far the
best performing hash table on all instances that make use of its
dynamic capabilities.  It performs up to TODO $\%$ better, than the
next best performing table.


.........


But there remains much to do. We were unable, to prove tight
theoretical performance and density bounds.

Even in practice, there remains a lot of open design space.  One could
use a similar approach to dynamically grow different cuckoo hashing
variants e.g. different variants of concurrent cuckoo
tables~\cite{AlgorithmicImprovementsForFastConcurrentCuckooHashing,
  LockFreeCuckooHashing}; or a table with de-amortized
insertions~\cite{UsingAQueueToDeAmortizeCuckooHashingInHardware,
  DeAmortizedCuckooHashingProvableWorstCasePerformanceAndExperimentalResults}.

............

%%
%% Bibliography
%%

%% Either use bibtex (recommended),

\bibliography{paper}

%% .. or use the thebibliography environment explicitely



\end{document}
