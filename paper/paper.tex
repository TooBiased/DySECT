\documentclass[a4paper,UKenglish]{lipics-v2016}
%This is a template for producing LIPIcs articles.
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
% for section-numbered lemmas etc., use "numberwithinsect"

\usepackage{microtype}%if unwanted, comment out or use option "draft"

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the recommended bibstyle

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Dynamic Space Efficient Cuckoo Table when doubling is too much -- a novel approach to growing!}
\titlerunning{A Sample LIPIcs Article} %optional, in case that the title is too long; the running title should fit into the top page column

%% Please provide for each author the \author and \affil macro, even when authors have the same affiliation, i.e. for each author there needs to be the  \author and \affil macros
\author[1]{Tobias Maier}
\author[1]{Dr. Peter Sanders}
\affil[1]{Karlsruhe Institute of Technology, Karlsruhe, Deutschland\\
  \texttt{\{t.maier, sanders\}@kit.edu}}
%% \affil[2]{Department of Informatics, Dummy College, Address/City, Country\\
%%   \texttt{access@dummycollege.org}}
\authorrunning{T. Maier and P. Sanders} %mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Tobias Maier and Peter Sanders}%mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{Dummy classification -- please refer to \url{http://www.acm.org/about/class/ccs98-html}}% mandatory: Please choose ACM 1998 classifications from http://www.acm.org/about/class/ccs98-html . E.g., cite as "F.1.1 Models of Computation".
\keywords{Dynamic data-structures, Open addressing, Closed hashing, Cuckoo hashing, Dense hash tables}% mandatory: Please provide 1-5 keywords
% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Acces}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{hyperref}
%\usepackage{graphix}



\begin{document}

\maketitle

\begin{abstract}
A lot of previous work concentrates on optimizing the speed and space
efficiency of hash tables.  Additionally many practical
implementations exist.  Modern open addressing techniques can fill a
given table to more than 95\% before the performance deteriorates
significantly.  But once the threshold is met, copying and migrating
the whole table still seems to be the most efficient answer.

To achieve any kind of efficiency, each such migration must be
amortized by newly inserted elements.  Therefore, each migration has
to increase the capacity by a significant amount -- usually, this
means doubling the capacity.  Growing by large factor
$\gamma$ often leads to bad space efficiency because the newly grown table
is only filled to less than a factor of $\gamma^{-1}$.

The space efficiency is even worse during the migration, where
oftentimes both the new and the old table coexist at the same time
causing a massive space overhead.  To solve both of these issues, we
propose the a new hash table architecture that supports dynamic
growing while keeping a strict size constraint $\alpha \cdot n$
throughout its lifetime.  Even under these conditions, we can
guarantee constant lookup times, and amortized insertion times,
independent of $n$.
 \end{abstract}

%% \section{Typesetting instructions -- please read carefully}
%% Please comply with the following instructions when preparing your article for a LIPIcs proceedings volume.
%% \begin{itemize}
%% \item Use pdflatex and an up-to-date LaTeX system.
%% \item Use further LaTeX packages only if required. Avoid usage of packages like \verb+enumitem+, \verb+enumerate+, \verb+cleverref+. Keep it simple, i.e. use as few additional packages as possible.
%% \item Add custom made macros carefully and only those which are needed in the article (i.e., do not simply add your convolute of macros collected over the years).
%% \item Do not use a different main font. For example, the usage of the \verb+times+-package is forbidden.
%% \item Provide full author names (especially with regard to the first name) in the \verb+\author+ macro and in the \verb+\Copyright+ macro.
%% \item Fill out the \verb+\subjclass+ and \verb+\keywords+ macros. For the \verb+\subjclass+, please refer to the ACM classification at \url{http://www.acm.org/about/class/ccs98-html}.
%% \item Take care of suitable linebreaks and pagebreaks. No overfull \verb+\hboxes+ should occur in the warnings log.
%% \item Provide suitable graphics of at least 300dpi (preferrably in pdf format).
%% \item Use the provided sectioning macros: \verb+\section+, \verb+\subsection+, \verb+\subsection*+, \verb+\paragraph+, \verb+\subparagraph*+, ... ``Self-made'' sectioning commands (for example, \verb+\noindent{\bf My+ \verb+subparagraph.}+ will be removed and replaced by standard LIPIcs style sectioning commands.
%% \item Do not alter the spacing of the  \verb+lipics-v2016.cls+ style file. Such modifications will be removed.
%% \item Do not use conditional structures to include/exclude content. Instead, please provide only the content that should be published -- in one file -- and nothing else.
%% \item Remove all comments, especially avoid commenting large text blocks and using \verb+\iffalse+ $\ldots$ \verb+\fi+ constructions.
%% \item Keep the standard style (\verb+plainurl+) for the bibliography as provided by the\linebreak \verb+lipics-v2016.cls+ style file.
%% \item Use BibTex and provide exactly one BibTex file for your article. The BibTex file should contain only entries that are referenced in the article. Please make sure that there are no errors and warnings with the referenced BibTex entries.
%% \item Use a spellchecker to get rid of typos.
%% \item A manual for the LIPIcs style is available at \url{http://drops.dagstuhl.de/styles/lipics-v2016/lipics-v2016-authors/lipics-v2016-manual.pdf}.
%% \end{itemize}

\section{Introduction}
Hash tables are some of the most frequently used data-structures. They
belong into every programmers basic toolbox.  As such having
interchangeable implementations which perform well under different
conditions is important and allows programmers
easily adapt known solutions to new circumstances.

One aspect which a lot of research focuses on is space efficiency.
Modern space efficient hash tables work well up to loads of 98\% and
more. But for this to matter, programmers have to know similarly tight
bounds to the absolute number of elements inserted into the table.
This is typically not the case, one of the most ubiquitous use cases
for hash tables is to unify data according to some key member.  To
guarantee good performance, we have to overestimate the necessary
capacity whenever the exact number of unique keys is not known a
priori.  Dynamic data-structures are necessary to alleviate this
problem.  In circumstances where space efficiency is important, space
efficiently growing hash tables are necessary.

Many libraries -- even ones that implement space efficient hash tables
-- already offer dynamic growing.  The problem with these
implementations is that they either lose their space efficiency or
their overall performance, once the table grows above its original
capacity.  Usually growing is implemented by either creating
additional hash tables -- decreasing the performance especially for
look ups, or by migrating all elements to a new table -- losing the
space efficiency by multiplying the original size.

To avoid both of these pitfalls we propose a variant of bucket cuckoo
hashing (TODO REFERENCE).  Bucket cuckoo hashing is a technique where
each element can be stored in one of several associated constant sized
buckets.  When all of them are full we move elements, to create new
space.  We split one bucket cuckoo table into multiple subtables each
of which can grow independent from all others.  Because the buckets
associated with one element are spread over the different subtables,
growing one subtable alleviates pressure from all other tables.

Doubling the size of one subtable increases the overall size by a
small factor, while only moving a small number of elements -- making
it possible, to amortize these small size changes. The introduced size
and occupancy imbalance between subtables is alleviated using
displacement techniques common to cuckoo hashing. Using these
techniques, makes our table work and grow efficiently with fill
degrees around 97\% (TODO READ APPROPRIATE VALUE) .  Being
approximately \ldots{} \% faster than common hash table techniques under the
same circumstances.

\section{Motivation}
Space efficiency is a major factor for the development of new hashing
techniques.  The goal has long been to make hash tables work better in
high load scenarios.  Modern hashing techniques like Hopscotch, Robin
Hood, and Cuckoo Hashing can easily fill hash tables to more than 90\%
and up to 98\% while still achieving constant average running times on
most of their operations.  But since these techniques depend on
statically sized tables this only makes sense, if the number of unique
elements can be estimated tightly.

To visualize this assume the following scenario.  During the operation
of a word count benchmark, we know an upper bound $c$ to the number of
unique words. Therefore, we construct a table large enough to hold
these elements ($0.9^{-1}\cdot c$).  It is entirely possible that the
specific word count instance only contains $0.7\cdot c$ unique words.
The resulting table is therefore only filled 63\%.  It is clear that
better static sized tables cannot solve this issue.

Depending on the underlying problem it is impossible to tightly
estimate the final number of elements.  Dynamic growing can be a
solution even in these instances.  It is completely independent from
the problem's domain and thus also works in cases where estimating the
number of elements can be hard.  But to achieve space efficiency in
scenarios where the final size is not known the hash table has to grow
closely with the actual number of elements.  This is not efficiently
possible with any of the growing techniques used currently.

\section{Related Work}
\begin{itemize}
\item Some other engineering papers

\item Some Space efficient Stuff  (theoretical)

\item Some Space efficient Stuff  (practical)
\end{itemize}

\section{Preliminaries}
A hash table is a data-structure, which stores key-value-pairs
($\langle key, data \rangle$) and offers the following functions:
\verb~insert~ stores a given key-value pair, \verb~find~ given a key returns if
said key was stored, and if so it returns the stored value, and
\verb~erase~ which deletes a previously inserted element (if present).

An alternative model which is sometimes used in literature considers
arbitrary elements instead of key value pairs.  In that model, the key
is extracted using an extractor function.  All techniques presented in
this paper also work in the extractor model without any adaptation.

Throughout this paper, we use $n$ to denote the number of elements and
$m$ the number of cells ($m > n$) in any given hash table.  We define
the load factor as $load = n/m$.  Tables can usually only work
efficiently up to a certain load factor.  Above that, operations get
slower and/or have a possibility to fail.
When implementing a hash table one has to decide between storing
elements directly in the hash table -- \emph{Closed Hashing} -- or storing
pointers to elements -- \emph{Open Hashing}. This has an immediate impact
on the necessary memory.

\begin{align*}
  mem_{closed}(n,m) &\geq^* m\cdot size(element)\\
  mem_{open}  (n,m) &\geq^* m\cdot size(pointer) + n\cdot size(element)\\
\end{align*}

For large elements ($\gg size(pointer)$), one can simply use open
hashing, to reduce the relevant memory factor.  Therefore, we restrict
ourselves to the common case of elements whose size is comparable to
that of a pointer.  For our experiments we use 128bit elements (64bit
keys and 64bit values).  In these instances closed addressing
introduces a significant memory overhead (at least $\times1.5$),
therefore, we only consider closed hash tables for the purpose of this
paper. For closed hash tables, the memory efficiency is directly
dependent on the element density.

\subsection{$\alpha$-Space Efficient Hash Tables}
We call a hashing technique $\alpha$-space efficient when it can
work effectively using at most $\alpha \cdot size_{\min} + O(1)$
memory (TODO DEFINE WORKING EFFICIENTLY FURTHER i.e. constant average
running time or just look at experiments or inserting the last 10\%
elements is less than inserting all others). In this case we define
working efficiently as having average insertion times close to
$O(\frac{m}{m-n})$ (average number of probes to hit an empty cell).

\[\alpha \cdot size_min = \alpha\cdot n\cdot size(element)\]

In many closed hashing techniques -- i.e. linear probing, cuckoo
hashing \ldots{} -- cells are the same size as elements. Therefore, being
$\alpha$ space efficient is the same as operating with a load factor
of $\alpha^{-1} = n/m$ ($\alpha\cdot n \cdot size(element) = m\cdot
size(element)$).  This is not necessarily the case for every kind of
hashing technique.  Some techniques like hopscotch hashing use
additional per cell information.  This would also be the case if we
store some per bucket data like the number of stored elements in our
bucket cuckoo hash table.

\subsubsection*{$\alpha$-Space Efficiency for Dynamic Tables}
The definition of a space efficient hashing technique given in (TODO
reference) the previous section is specifically targeted for
statically sized hash tables.  As detailed in Motivation (TODO
REFERENCE) our goal is to construct a hash table which can efficiently
(and tightly) grow with the number of inserted elements. Therefore, we
want to define the notion of an $\alpha$-space efficient dynamic
table.

We call a hash table implementation dynamically $\alpha$-space
efficient if an instantiated table can grow arbitrarily large over its
original capacity, while remaining smaller than $\alpha\cdot
mem_{necessary}$ at all times.
\[mem_{curr} \leq \alpha\cdot max(n_{curr})\cdot size(element)\]

One point of discussion while defining dynamic space efficiency is the
space consumption during hash table operations.  During a table
migration both the source ($t_{source}$) and the target
($t_{target}$) table are allocated, thus using $mem =
(m_{source}+m_{target})\cdot size(cell)$ space.  This is especially
prohibiting if all elements are held in one table, then at least
$2\cdot m\cdot size(element)$ is necessary for each migration.

Our definition of dynamic space efficiency does not enforce size
reduction due to the deletion of elements, but it enforces that the
memory used by deleted elements must be reused (no tombstones).

\paragraph*{Trivial Examples}
One simple example of this is a linear probing table, which grows by a
factor of two, whenever 50\% of the table is filled. Here the table is
at least 25\% filled (immediately after growing), therefore, the table
is 6-space efficient (during the grow operation there are $(4+2)+n$
cells). Another example would be a cuckoo table which doubles in size
whenever it surpasses 95\%, fill rate. This would be a
$0.95^{-1}+0.45^{-1}\approx 3.2$-space efficient table.

\subsection{Cuckoo Hashing}
Cuckoo hashing is a technique to resolve hash conflicts in an closed
hashing hash table. Its main draw is that it guarantees constant
lookup times even in densely filled tables. The article TODO by TODO
(probably mitzenmacher with or without dietzfelbinger) et al. gives a
good overview over many cuckoo hashing variants.  The distinguishing
technique of cuckoo hashing is, that $H$ hash functions ($h_1, ... ,
h_H$) are used to find $H$ independent possible positions. Each
element is stored in one of its positions.  Even if all possible
positions are occupied one can often move elements to create space for
the current element. We call this displacing elements.

Bucket cuckoo hashing is a variant, where the cells of the hash table
are grouped into buckets of size $B$ ($m/B$ buckets).  Each element
appointed to one bucket can be stored in any of the bucket's cells .
Using buckets one can increase the number of displacement
opportunities drastically.

\textbf{Lookup:} Find and Remove operations have a guaranteed constant
running time. Independent from the tables density, there are $H$
possible buckets -- $H\cdot B$ cells -- that have to be searched to
find an element.

\textbf{Insert:} Each element is hashed to $H$ buckets.  If at least
one of those buckets has space left, then we store the element in the
bucket, that has the most space.  But when all buckets are full, then we
have to move elements around the table, such that a new space becomes
available.

To visualize the problem of displacing elements, one can think of the
directed graph implicitly defined by the hash table.  Each bucket
defines one node and each element defines pairwise connections between
the bucket it is stored in to its $H-1$ alternate buckets.  To insert
an element into the hash table we have to find a path from one of its
associated buckets, to a bucket, that has space remaining.  Then we
move elements along this path, to open a space on the initial bucket.
The two common techniques to find such paths are \emph{random walks} and
\emph{breadth first searches}.

\subsubsection*{Some Performance Bounds}
\begin{itemize}
\item bla how dense can we get
\end{itemize}

When randomly probing the table we expect to find a free cell after $\frac{1}{1-load factor}$

\section{DySECT Space Efficient Growing, by Splitting the Table}
Our goal is to build an efficiently growing data structure, that
remains space efficient at all times.  A commonly used growing
technique is to increase the size by migrating all elements
into a larger new table.  Of course this is
not memory efficient.  A natural thought which occurs is to double
only part of the data structure.  This idea together with the
balancing ability of multiple hash functions is what will achieve the
described functionality.

\subsection{Description}
We use multiple ($T$) subtables (shown in TODO FIGREFRENCE) to allow
differentiated growing between table parts.  The subtables consist of
buckets that store elements.

Comparable to classic cuckoo hashing, each element has $H$ associated
buckets ($H$ number of hash functions), these can be in the same, or
in different subtables.  Each inserted element lies in one of its
associated buckets.  To find a bucket associated with an
element $e$, we compute $e$s hash value using the appropriated hash function
$h_i(e)$. We split that hash value into two parts, then we use the
first part to chose a subtable, and the second part to
choose one of the contained buckets.

%% To understand the performance of this hash table, it is important to
%% realize that when the hash table is used regularly, the first level
%% table will always remain cached. This is important, because otherwise lookups
%% would cause unnecessary cache misses that would not have happened in a
%% single level hash table.  Because of this we expect the average number
%% of cache misses during hash table operations to be similar to those of
%% a normal cuckoo hash table.

\subsection{Growing}
When the table contains enough elements, that the memory constraint
can be kept, we migrate a subtable.  We migrate subtables in order
from the first to the last, therefore, no subtable can be more than
twice as large as any other.

Assume that all subtables have $s=m/T$ cells. When $\alpha\cdot n >
m+2s$ We can grow the first subtable while keeping the size
constraint.  Doubling the size of a subtable increases the global
number of cells from $m_{old} = T\cdot s$ to $m_{new} = m_{old}+s =
(T+1)\cdot s$ (factor: $\frac{T+1}{T}$).  Note that all subsequent
grows migrate one of the smaller tables, until all tables have the
same size again.  Therefore, each grow until then increase the
capacity by the same absolute amount (smaller factor).

From a theoretical point of view, the cost of growing the subtable is
amortized by element insertions.  There are at least $\alpha^{-1}
\cdot s = \Omega(s)$ insertions between two migrations.  One migration
takes $\Theta(s)$ time. Alternatively abstract point of view, after
inserting $\alpha^{-1} \cdot s$ elements the table grows enough, to
raise the capacity about the same $\alpha^{-1} \cdot s$ effective
cells (actual cells $\times$ fill factor).

From a practical view, the migration is efficient because it accesses
cells in a linear fashion making it really cache efficient.  Even in
the target table cells are accessed linearly, because of the way we
assign elements to buckets there are no displacements necessary.  The
elements from each original bucket are split into two buckets of the
target table.  Therefore, no bucket of the target table can have more
elements than its respective original bucket.

In the implicit graph model of the cuckoo table (TODOsee section
cuckoo hashing), growing a subtable is equivalent to
splitting each node that represents a bucket within that table. The
edges (elements) in the implicitly defined subgraph are not doubled,
therefore, the resulting subgraph becomes more sparse, making it
easier to insert elements.

\subsection{Shrinking}
In many use cases, shrinking is not necessary.  It worsens performance
by taking time for the migration and making the remaining table more
dense.  Furthermore, it is unclear that the freed memory is actually
necessary for other parts of the application.  If that memory is
necessary, and shrinking is required, for example, when removed
elements are reinserted into another data structure and global memory
bounds are necessary (other data structures grow while the hash table
shrinks).

Shrinking can work similarly to growing. We replace a subtable with a
smaller one by migrating elements from one to the other.  During this
migration we join elements from two buckets into one. Therefore, it is
possible for a bucket to overfill.  We reinsert these elements at the
end of the migration.  Obviously, this can only apply to at most half
the migrated elements.

When triggering the size reduction, one has to make sure that the
migration is amortized. Therefore, a grow operation cannot immediately
follow a shrink operation.  When shrinking is enabled we propose to
shrink one subtable when $\alpha*n < m-s'$ elements ($s'$ size of a
large table, $m_{new} = m_{old} -s'/2$).

Notice that the memory requirement actually increases during the
operation itself.
\subsection{Hashing Elements to Buckets}
\subsubsection*{Compute the Bucket from a Hashed Key}
From a hashed key, we have to compute the subtable and the bucket
within that subtable.  To make this efficient we use powers of two for
the number of subtables ($T = 2^t$), as well as the individual
subtable sizes ($s = 2^*$).  Since the number of subtables remains
constant we can use the first $t$ bits from the hashed key, to find the
appropriate subtable.  From the remaining bits we compute the offset
into this subtable using a bitmask ($\texttt{AND}~s-1 \leftrightarrow
\mod s$).

\subsubsection*{Reducing Number Computed Hash Functions}
Evaluating hash functions is expensive, therefore, reducing the number
of hash functions computed per operation increases the efficiency of
the table.  The hash function we use computes 64bit hash values
(i.e. xxHash TODO).  We split these 64bit value into two 32bit hash
values. All common bucket hash table sizes can be addressed using 32
bits. They can address up to $2^{32}$ buckets which achieves a
capacity for $2^{35} \approx 34$ billion elements (bucket size 8),
at that point a hash table consumes 512GiB memory.

When $H > 2$ we can use \emph{double hashing} to further reduce the number
of computed hash functions. Double hashing creates an arbitrary amount
of hash values using only two original hash functions $h'$ and
$h''$.  The additional values are linear combinations computed
from the original two values, $h_i(key) = h'(key) + i\cdot
h''(key)$.

Combining both of these techniques, we can reduce the number of
computed hash functions to one 64bit hash function.  This is
especially important during large displacements, where each
encountered element has to be rehashed to find its alternative
buckets.

\subsection{Difficulties for the Analysis of our table structure}
There are three factors, that impact the performance of our dynamic
table compared to other cuckoo table variants and to other hashing solutions in
general \textbf{inhomogeneous table resolution}, \textbf{element imbalance}, and
\textbf{population density}. All of these factors influence the maximum load
density and the running times in different ways.

\subsubsection*{Imbalance through Inhomogeneous Table Resolution}
\label{sec:inhom_res}
By growing subtables individualy we introduce a size imbalance
between subtables.  Large tables contain more buckets, but the number
of elements hashed to the table itself is not dependent on its size,
therefore, it is difficult to spread elements evenly among buckets.
Uneven bucket fill ratios can lead to longer insertion times.

If there are $n$ elements in a hash table with $T$ subtables,
$j$ of which have size $2s$ the others have size $s$. If elements are
spread equally among buckets then all small tables have around
$n/(T+j)$ elements, and the bigger tables have $2n/(T+j)$ elements.
For each table there are about $Hn/T$ elements associated to one of
its buckets.  This shows that having more hash functions can lead to a
better balance.

For two hash functions ($H=2$) and only one grown table ($j=1$) this
means that $\approx 2n/(T+1)$ should be stored in the first table.
These are nearly all elements associated to a bucket in the first
table ($\approx 2n/T$). So to distribute elements evenly nearly all
elements would have to be stored there.

\subsubsection*{Imbalance through Size Changes}
In addition to the problem of inhomogenous tables, there is an
inherent balancing problem introduced by resizing subtables. It is
clear, that a newly grown table is not filled as densely as other
tables.  Since we double the table size, grown tables can only be
filled to about 50\%.

Assume the global table is filled close to 100\% when the first table begins
to grow.  now there is the capacity for $s$ new elements, but this
capacity is only in the first table, elements that are not hashed to
the first table, automatically trigger displacements leading to slow
insertions.

Notice that repeated operations help to equalize this imbalance,
because elements are more likely inserted into the less dense areas,
and more likely to be deleted from more dense areas.

\subsubsection*{Population Density}
Measuring and comparing the performance of space efficient growing
tables to their statically sized counterparts is difficult.  Inserting
elements into a densely filled table takes longer, than into an empty
one.  Therefore, a table that is always densely filled will naturally
be slower than one that only fills up towards the end of the
execution -- even when ignoring eventual growing costs.

\section{Experiments}
As described in section (TODO REFERENCE DIFFICULTIES\ldots{}) there are
many factors that impact hash table performance.  Many of these
factors are inherent to the combination of growing and space
efficiency.  To show that our ideas work from a practical point of
view we use a wide portfolio of both micro-benchmarks and practical
experiments.

\subsection{Methodology and Comparison Implementations}

\subsubsection*{Choice of Constants}
In our tests, we use three hash functions ($H=3$) and a bucket size of
($B=8$). These values have consistently outperformed other
possibilities.  We chose to use 256 subtables for all our tests
($T=256$).  The number of subtables only plays a minor role for the
running time.

\subsubsection*{Comparison Implementations}
To put the performance of our solution into perspective, we
implemented and tested several other options for space efficient
hashing.  We use our own implementations, since most hash tables found
online do not support strict space-efficiency.  In Appendix TODO we
show that our implementations are at least competitive with
implementations found .

\paragraph*{Linear Probing} is the most common hashing technique outside of
hashing with chaining.  It is commonly accepted to be inefficient for
higher load factors because search distances can become large.  In
practice, this fact is counteracted by its cache-efficiency making
operations fast in spite of long probing distances.  Linear probing is
especially slow in \verb~find~ and \verb~erase~ operations, that often have
guaranteed constant running time in other implementation.

To grow our linear probing table, we use table migrations.  This can
be made space efficient, by increasing the capacity in place.  This is
possible due to over allocation (MAYBE SOME SOURCE/APPENDIX/WRITE
SHORT PAPER) and a simple in place migration algorithm (ALSO
APPENDIX/SHORT PAPER).  For a table that was initialized to hold $n'$
elements with a space-efficiency $\alpha$ we trigger growing once the
number of elements is $n = \frac{(1+\alpha)}{2}n'$. We then increase
the capacity $m$ to $\alpha\cdot n$ to remain strictly size-efficient.
Our implementation of the migration is highly cache efficient, but
all elements have to be moved.  This makes repeated migrations inefficient
especially for small $\alpha$.

\paragraph*{Hopscotch Hashing} is a technique similar to linear probing where
elements are moved when they are too far from their original hash
position.  Additionally, hopscotch hashing introduces an acceleration
data structure which uses bitmaps to store neighborhood data. This
data improves find and erase performance.  Many people would expect
the acceleration data to be the main draw to hopscotch hashing, but it
is also its greatest demise in the space-efficient setting.  The
bitmaps introduce a significant memory overhead of neighborhood-size
bits per cell.  Our experiments show that for any reasonable fill
factor neighborhoods have to be 64bits and larger, this introduces a
size factor of $\times1.5$ (128bit elements).  We still include
measurements with hopscotch hashing -- without considering space for
the neighborhood data -- to show its overall performance tendencies.
Growing hopscotch tables is done similar to the technique described
under linear probing.

\paragraph*{Robin Hood Hashing} is another technique based on linear probing. The
goal of this technique is to improve find performance by minimizing
the maximum/average displacement distance.  The drawback of this technique is
that insertions have to compute the hash of each element that is
skipped by the insertion.

\paragraph*{Classic Bucket Cuckoo Hashing} is the technique we adopted to
design our hashing technique.  It has clear advantages for space
efficient tables.  Like our variant it offers constant lookup and
deletion times.  In contrast to our variant, it does not suffer from
any imbalances caused either by table resolution or by local growing
operations.  Growing works similar to the linear probing table above.
Similar to DySECT, $H=3$ and $B=8$ have turned out to be an optimum
for performance.

\subsubsection*{Hardware}
We test our implementation on two very different systems, first a
consumer level desktop computer \ldots{}

\subsection{Influence of Fill Ratio under a Static Table Size}
\label{sec:exp_eps}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/eps_insert.pdf}
  \caption{\label{fig:eps_insert}Insert timings at different fill
    degrees into a non-growing table.}
\end{figure}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\textwidth]{../eval/plots/eps_find_s.pdf}
  \includegraphics[width=0.49\textwidth]{../eval/plots/eps_find_u.pdf}
  \caption{Timings for find operations with different fill degrees
    (\emph{left:}successful, \emph{right:}unsuccessful).}
  \label{fig:eps_find}
\end{figure}
Our hash table is designed for space efficient growing, therefore, its
main advantages show in dynamic workloads.  We still performed some
tests with a static table size.  With these measurements, we can
highlight different points without the influence of dynamically growing.

The following test was performed, by initializing a table with
$m\approx24M$~cells (non-growing). Then elements are inserted, until there
is a critical amount of failing insertions ($>100$).  At different
stages, we measure the speed of new insertions
(Figure~\ref{fig:eps_insert}), and finds (Figure~\ref{fig:eps_find}).
Performance is measured by performing each operation 1000 times, finds
are measured using either random elements within the table (successful), or by
searching random elements from key space (unsuccessful).

As one might expect, the performance, of insert operations depends
highly on the fill degree of the table.  Therefore, we show it
normalized with $\frac{1}{\varepsilon}$, which is the expected number
of fully random probes, to find a free cell. We see that -- up to a
certain point -- for all tables the insertion time behaves
proportional to $\frac{1}{\varepsilon}$, then it begins to decrease
uncontrolled.  The plot shows, that our table remains stable for
longer than linear probing variants, but not as long, as for classic
cuckoo hashing.  Hopscotch hashing fails earlier than all other
tables, because its bitmaps become to small, to store the neighborhood
information.

DySECTs performance degrades earlier than cuckoo hashing.  The cause
for this is twofold. First, our architecture -- $T$ subtables with power
of two sizes -- does not allow exactly
$24M$ cells , therefore, our table is slightly more full. Secondly, to
get close to $24M$ cells, we have $110$ large subtables (twice the
size of the others).  This introduces inhomogeneous table resolution
(see \ref{sec:inhom_res}), which decreases our performance compared to
cuckoo hashing.  This test shows the influence of inhomogeneous table
resolution without the influence of size changes, and the connected
table imbalance.

Figure~\ref{fig:eps_find} shows that linear probing variants have bad
performance on find operations on highly filled tables.  It also shows
that unsuccessful find operations are often very slow when using
linear probing.  This is where the acceleration data-structure used by
hopscotch tables can significantly improve performance.  Both of these
effects, do not affect cuckoo variants like DySECT, that have
guaranteed constant running times for all find operations --
independant of the fill degree.

\subsection{Constructing a Table with Growing}
\label{sec:exp_ti}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/ti_insert.pdf}
  \caption{\label{fig:ti_insert}Average time per insert operation
    during the construction of a dynamic table.}
\end{figure}
%% \begin{figure}
%%   \includegraphics[width=0.48\textwidth]{../eval/plots/ti_find_s.pdf}
%%   \includegraphics[width=0.48\textwidth]{../eval/plots/ti_find_u.pdf}
%%   \caption{Timings for find operations on the grown table (from Figure~\ref{fig:ti_insert}; \emph{left:}successful, \emph{right:}unsuccessful).}
%%   \label{fig:ti_find}
%% \end{figure}
In this test $20M$ elements are inserted into a previously empty
table. All tables are initialized expecting 50\,000 elements, thus
growing is necessary, to fit all elements.  The tables are configured,
to guarantee a load factor of
$\varepsilon$. Figure~\ref{fig:ti_insert} shows the performance in
relation to $\varepsilon$.  Insertion times are normalized similar to
Figure~\ref{fig:eps_insert} (divided by $\frac{1}{\varepsilon}$) to
make them more readable.

We see that DySECT performs by far the best even with less filled
tables.  Here we achieve a speedup of TODO$\%$ over the next best
solution (blabla vs. linear probing blabla at $\varepsilon = 0.$ bla).
On denser instances, we can increase this margin to TODO$\%$ (blabla
vs we'll see blabla at $\varepsilon = 0.$ bla).  With shrinking
$\varepsilon$, we see that insertion times start to degrade for our
competitors.  DySECT remains close to $O(\frac{1}/{\varepsilon})$ even
for fill degrees up to 97.5$\%$ (TODO check).

We also measured the performance of find operations on the created
tables, they are similar to the performance on the static table in
Section~\ref{sec:exp_eps} (see Figure~\ref{fig:eps_find}), therefore,
we omit plotting them for space reasons.

\subsection{Word Count a Practical use Case}
One of the most common use cases for hash tables -- dynamic hash
tables in particular -- is to unify data elements with the same key.
Oftentimes it is unclear how many unique keys are present (how large
the table will get).  In these cases, only dynamic hash tables can
remain space efficient.  In the following test, we take the first
block of the common crawl data-set (TODO REFERENCE) and compute a word
count of the contained words.  The chosen block contains around 240M
words, with around 20M unique words.

For the test, we hash each word and insert it together with a counter.
Subsequent insertions of a reocurring word increase this counter.
Similar to the growing benchmark, we start with an empty table
initialized for 50\,000 elements.  The performance results can be seen
in Figure~\ref{fig:crawl}.  We do not use any normalization since --
on average -- each word is repeated 12 times, therefore, most inserts
will actually behave like successful find operations (returning an
iterator to the contained element).  Our DySECT table manages to
dominate the field, similar to the growing test in
Section~\ref{sec:exp_ti}.  This is a hint to the fact, that insert
performance is important even in find intensive workloads.  This
insight is intensified in the following test
(Section~\ref{sec:exp_mix}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/crawl.pdf}
  \caption{\label{fig:crawl} }
\end{figure}

\subsection{Combining insert and find operations}
In this test, we simulate an application in progress.  We use tables
with a minimum load factor of $\varepsilon = 0.9xx$ (TODO choose one).
The test starts a decently filled table (TODO M elements).  On this
table we perform a mixture of insert and find operations.  Even on




%%
%% Bibliography
%%

%% Either use bibtex (recommended),

\bibliography{lipics-v2016-sample-article}

%% .. or use the thebibliography environment explicitely



\end{document}
