% -*- mode: latex; mode:writegood mode: flyspell; ispell-local-dictionary: "en_US"; coding: utf-8 -*-

\documentclass[a4paper,UKenglish]{lipics-v2016}
%This is a template for producing LIPIcs articles.
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
% for section-numbered lemmas etc., use "numberwithinsect"

\usepackage{microtype}%if unwanted, comment out or use option "draft"

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the recommended bibstyle

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{DySECT -- truly space efficient dynamic hashing} % -- a novel approach to growing!}
\titlerunning{A Sample LIPIcs Article} %optional, in case that the title is too long; the running title should fit into the top page column

%% Please provide for each author the \author and \affil macro, even when authors have the same affiliation, i.e. for each author there needs to be the  \author and \affil macros
\author[1]{Tobias Maier}
\author[1]{Peter Sanders}
\affil[1]{Karlsruhe Institute of Technology, Karlsruhe, Deutschland\\
  \texttt{\{t.maier, sanders\}@kit.edu}}
%% \affil[2]{Department of Informatics, Dummy College, Address/City, Country\\
%%   \texttt{access@dummycollege.org}}
\authorrunning{T. Maier and P. Sanders} %mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Tobias Maier and Peter Sanders}%mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{Dummy classification -- please refer to \url{http://www.acm.org/about/class/ccs98-html}}% mandatory: Please choose ACM 1998 classifications from http://www.acm.org/about/class/ccs98-html . E.g., cite as "F.1.1 Models of Computation".
\keywords{Dynamic data-structures, Open addressing, Closed hashing, Cuckoo hashing, Dense hash tables}% mandatory: Please provide 1-5 keywords
% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Acces}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{hyperref}
%\usepackage{graphix}



\begin{document}

\maketitle

\begin{abstract}
Research has concentrated on optimizing the speed and space efficiency
of hash tables for a long time.  Modern open addressing techniques can
fill a given table to more than 95\% before the performance
deteriorates significantly.  But once the threshold is met, copying
and migrating the whole table still seems to be the most efficient
answer.  Growing by a constant factor $\gamma$ often leads to bad
space efficiency because the newly grown table is only filled less
than a factor of $\gamma^{-1}$.

The space efficiency is even worse during the migration, where
oftentimes both the new and the old table coexist at the same time
causing a massive space overhead.  To solve both of these issues, we
propose the a new hash table architecture that supports dynamic
growing while keeping a strict size constraint $\alpha \cdot n$
throughout its lifetime.  Even under these conditions, we achieve
guaranteed constant lookup times, and amortized insertion times,
independent of $n$.

We achieve this goal by splitting the hash table into subtables that
grow independent from each other.  The cuckoo load balancing mechanism
can move elements from one table to the other.  This ensures that
growing one subtable increases insertion opportunities even for
elements that are not hashed into this one table.
 \end{abstract}

%% \section{Typesetting instructions -- please read carefully}
%% Please comply with the following instructions when preparing your article for a LIPIcs proceedings volume.
%% \begin{itemize}
%% \item Use pdflatex and an up-to-date LaTeX system.
%% \item Use further LaTeX packages only if required. Avoid usage of packages like \verb+enumitem+, \verb+enumerate+, \verb+cleverref+. Keep it simple, i.e. use as few additional packages as possible.
%% \item Add custom made macros carefully and only those which are needed in the article (i.e., do not simply add your convolute of macros collected over the years).
%% \item Do not use a different main font. For example, the usage of the \verb+times+-package is forbidden.
%% \item Provide full author names (especially with regard to the first name) in the \verb+\author+ macro and in the \verb+\Copyright+ macro.
%% \item Fill out the \verb+\subjclass+ and \verb+\keywords+ macros. For the \verb+\subjclass+, please refer to the ACM classification at \url{http://www.acm.org/about/class/ccs98-html}.
%% \item Take care of suitable linebreaks and pagebreaks. No overfull \verb+\hboxes+ should occur in the warnings log.
%% \item Provide suitable graphics of at least 300dpi (preferrably in pdf format).
%% \item Use the provided sectioning macros: \verb+\section+, \verb+\subsection+, \verb+\subsection*+, \verb+\paragraph+, \verb+\subparagraph*+, ... ``Self-made'' sectioning commands (for example, \verb+\noindent{\bf My+ \verb+subparagraph.}+ will be removed and replaced by standard LIPIcs style sectioning commands.
%% \item Do not alter the spacing of the  \verb+lipics-v2016.cls+ style file. Such modifications will be removed.
%% \item Do not use conditional structures to include/exclude content. Instead, please provide only the content that should be published -- in one file -- and nothing else.
%% \item Remove all comments, especially avoid commenting large text blocks and using \verb+\iffalse+ $\ldots$ \verb+\fi+ constructions.
%% \item Keep the standard style (\verb+plainurl+) for the bibliography as provided by the\linebreak \verb+lipics-v2016.cls+ style file.
%% \item Use BibTex and provide exactly one BibTex file for your article. The BibTex file should contain only entries that are referenced in the article. Please make sure that there are no errors and warnings with the referenced BibTex entries.
%% \item Use a spellchecker to get rid of typos.
%% \item A manual for the LIPIcs style is available at \url{http://drops.dagstuhl.de/styles/lipics-v2016/lipics-v2016-authors/lipics-v2016-manual.pdf}.
%% \end{itemize}

\section{Introduction}
Hash tables are some of the most frequently used data-structures. They
belong into every programmers basic toolbox.  As such having
interchangeable implementations which perform well under different
conditions is important and allows programmers to easily adapt known
solutions to new circumstances.

One aspect which a lot of research focuses on is space efficiency.
Modern space efficient hash tables work well up to loads of 95\% and
more. But for this to matter, programmers have to know similarly tight
bounds to the absolute number of elements inserted into the table.
This is typically not the case, one of the most ubiquitous use cases
for hash tables is to unify data elements by their key.  To guarantee
good performance, we have to overestimate the initial capacity
whenever the exact number of unique keys is not known a priori.
Dynamic data-structures are necessary to alleviate this problem.  In
circumstances where space efficiency is important, we need space
efficiently growing hash tables.

Many libraries -- even ones that implement space efficient hash tables
-- already offer dynamic growing.  The problem with these
implementations is that they either lose their space efficiency or
their overall performance, once the table grows above its original
capacity.  Usually growing is implemented by either creating
additional hash tables -- decreasing the performance especially for
look ups, or by migrating all elements to a new table -- losing the
space efficiency by multiplying the original size.

To avoid both of these pitfalls we propose a variant of bucket cuckoo
hashing (TODO REFERENCE).  Bucket cuckoo hashing is a technique where
each element can be stored in one of several associated constant sized
buckets.  When all of them are full we move elements, to create new
space.  To solve the problem of efficient migration, we split the
table into multiple subtables each of which can grow independently
from all others.  Because the buckets associated with one element are
spread over the different subtables, growing one subtable alleviates
pressure from all others by allowing moves from a dense table to the
grown table.

Doubling the size of one subtable increases the overall size by a
small factor, while only moving a small number of elements -- making
the size changes easy to amortize. The introduced size and occupancy
imbalance between subtables is alleviated using displacement
techniques common to cuckoo hashing. Using these techniques, makes our
table work and grow efficiently with fill degrees of 95\% and more.
Being approximately TODO \ldots{} \% faster than common hash table
techniques under the same circumstances.

\section{Motivation}
\label{sec:motivation}
Space efficiency is a major factor for the development of new hashing
techniques.  The goal has long been to make hash tables work better in
high load scenarios.  Modern hashing techniques like Hopscotch, Robin
Hood, and Cuckoo Hashing can easily fill hash tables to more than 90\%
and up to 98\% while still achieving constant average running times on
most of their operations.  But since these techniques depend on
statically sized tables this only makes sense, if the number of unique
elements can be estimated with similarly tight bounds.

To visualize this assume the following scenario.  During the operation
of a word count benchmark, we know an upper bound $c$ to the number of
unique words. Therefore, we construct a table large enough to hold
these elements ($0.9^{-1}\cdot c$ cells).  It is entirely possible
that the specific word count instance only contains $0.7\cdot c$
unique words (or less).  The resulting table is therefore only filled
63\%.  It is clear that better static sized tables cannot achieve fill
ratios greater than 70\%.

Depending on the underlying problem it is impossible to tightly
estimate the final number of elements. Since dynamic growing is independent from the problem's domain, it can be a
solution even in these instances.  To achieve space efficiency in
scenarios where the final size is not known the hash table has to grow
closely with the actual number of elements.  This is not efficient
with any of the current techniques used for hashing and migration.

\section{Related Work}
The usage of hash tables, and other hashing based algorithms has a
long history in computer science.  The classical methods and results
can be found in all major algorithm textbooks (TODO ...).

Over the last one and a half decades, the field has regained
attention, both from theoretical and the practical point of view.  The
initial innovation that sparked this attention was the power of two
choices~\cite{ThePowerOfTwoChoicesInRandomizedLoadBalancing}.  This
concept led to the development of cuckoo hashing~\cite{CuckooHashing}
which reopened research into more space efficient hash tables.  They
proofed theoretically interesting, because probabilistic bounds for
the maximum fill degree are often highly non-trivial TODO SOME BOUNDS
PAPERS.

Cuckoo hashing also spawned many variants, that further improve its
performance.  The most notable one is probably bucket cuckoo hashing
($d$-ary cuckoo hashing), a variant that groups cells together in
buckets thus improving cache usage, and increasing displacement
opportunities TODO BUNCH OF PAPERS.  This further improves the maximum
fill degree. Nowadays, bucket cuckoo hashing is by far the most common
variant. There are even further variations on bucket cuckoo hashing,
like the generalization to continuous buckets TODO PAPER.

Further adaptations to cuckoo hashing include: a de-amortization
technique that allows a provable worst case analysis of
insertions~\cite{UsingAQueueToDeAmortizeCuckooHashingInHardware,
  DeAmortizedCuckooHashingProvableWorstCasePerformanceAndExperimentalResults};
and multiple concurrent implementations either powered by bucket
locking, transactional
memory~\cite{AlgorithmicImprovementsForFastConcurrentCuckooHashing},
or fully lock-less~\cite{LockFreeCuckooHashing}.

Some non-cuckoo space efficient hash tables continue to use linear
probing variants.  Robin Hood hashing is a technique that was
originally introduced in 1985~\cite{RobinHoodHashing}. It has regained some popularity in
recent years -- mainly for its interesting theoretical
properties and the possibility to reduce the inherent variance of
linear probing.  Hopscotch~\cite{HopscotchHashing} hashing is a
technique, that originated in the research of concurrent hash tables, but
it can also lead to some densely filled hash tables.  For this reason
it is often used as a technique to achieve space efficient hash
tables.  This is only possible in a small number of circumstances, as
the stored bitmaps used in hopscotch hashing create a significant
memory overhead.

All these publications show that there is a clear interest in the
development of space efficient hash tables.  Dynamic hash tables on the
other hand seem to be considered a solved problem.  There is an
important paper by Dietzfelbinger at
al.~\cite{DynamicPerfectHashingUpperAndLowerBounds} that takes on the
problem of dynamic hash tables from a theoretical perspective.  The
paper predates cuckoo hashing, and much of the attention for space
efficient hashing. Thus all memory bounds presented are given without
tight constant factors.  This lack of development is where we pick up
and offer fast solutions that support dynamic growing with tight space
bounds.

\section{Preliminaries}
A hash table is a data-structure, which stores key-value-pairs
($\langle key, data \rangle$) and offers the following functions:
\verb~insert~ -- stores a given key-value pair; \verb~find~ -- given a
key returns if said key was stored, and $\bot$ otherwise; and
\verb~erase~ -- deletes a previously inserted element (if present).

An alternative model which is sometimes used in literature considers
arbitrary elements instead of key value pairs.  In that model, the key
is extracted using an extractor function.  All techniques presented in
this paper also work in the extractor model without any adaptation.

Throughout this paper, we use $n$ to denote the number of elements and
$m$ the number of cells ($m > n$) in any given hash table.  We define
the load factor as $load = n/m$, and we use $\varepsilon = 1-load$ for
the fraction of available cells.  Tables can usually only work
efficiently up to a certain load factor.  Above that, operations get
slower and/or have a possibility to fail.  When implementing a hash
table one has to decide between storing elements directly in the hash
table -- \emph{Closed Hashing} -- or storing pointers to elements --
\emph{Open Hashing}. This has an immediate impact on the necessary
memory.

\begin{align*}
  mem_{closed}(n,m) &\geq m\cdot size(element)\\
  mem_{open}  (n,m) &\geq m\cdot size(pointer) + n\cdot size(element)\\
\end{align*}

For large elements ($\gg size(pointer)$), one can use a non-space
efficient hash table with open hashing, to reduce the relevant memory
factor.  Therefore, we restrict ourselves to the common and more interesting case of
elements whose size is comparable to that of a pointer.  For our
experiments we use 128bit elements (64bit keys and 64bit values).  In
these instances open hashing introduces a significant memory
overhead (at least $\times1.5$), therefore, we only consider closed
hash tables for the purpose of this paper. For closed hash tables, the
memory efficiency is directly dependent on the element density.  To
reach high fill degrees with closed hashing tables, we have to employ
\emph{open addressing} techniques which means, that elements are not
stored in predetermined cells, but can be stored in one of several
options (e.g. linear probing, or cuckoo hashing).

\subsection{$\alpha$-Space Efficient Hash Tables}
\subparagraph*{Static}
\label{sec:pre_staticspace}
We call a hashing technique $\alpha$-space efficient when it can work
effectively using at most $\alpha \cdot n_{\textit{curr}} \cdot \textit{size}(\textit{element}) + O(1)$ memory. In
this case we define working efficiently as having average insertion
times close to $O(\frac{1}{\varepsilon})$. This is a natural bound
since it is the expected number of fully random probes needed to hit
an empty cell.

%\[\alpha \cdot size_min = \alpha\cdot n\cdot size(element)\]

In many closed hashing techniques (e.g.~linear probing, cuckoo
hashing) cells are the same size as elements. Therefore, being
$\alpha$ space efficient is the same as operating with a load factor
of $\alpha^{-1} = n/m$.
%(because $\alpha\cdot n \cdot size(element) = m\cdot size(element)$).
Because of this, we will mostly talk about the load factor of a table
instead of its memory usage.

Some techniques like hopscotch hashing use additional per cell
information.  These hash table architectures, have to be filled more
densely to achieve the same space efficiency.

\paragraph*{Dynamic}
The definition of a space efficient hashing technique given aboce is
specifically targeted for statically sized hash tables.  We call a
hash table implementation dynamically $\alpha$-space efficient if an
instantiated table can grow arbitrarily large over its original
capacity, while remaining smaller than $\alpha\cdot n_{\max}\cdot
\textit{size}(\textit{element}) + O(1)$ at all times.
%\[mem_{curr} \leq \alpha\cdot max(n_{curr})\cdot size(element)\]

One problem for many implementations of space efficient hash tables is
the migration.  During a normal full table migration, both the
original table, and the new table are allocated.  This takes
$\textit{m}_{\textit{new}} + \textit{m}_{\textit{old}}$
cells. Therefore, no normal full table migration can be more then
2-space efficient.  The only option to perform a full table migration
with less memory, is to increase the memory in place (see
Section~\ref{sec:dys_inplace} and
Section~\ref{sec:exp_competitor}).

Similar to static $\alpha$-space efficiency, we will mostly talk about
the \emph{minimum load factor}, instead of $\alpha$.  We should note,
that this definition of dynamic $\alpha$-space efficient hash tables
does not enforce size reduction due to the deletion of elements, but
it enforces that cells previously used by deleted elements must be reused.

\subsection{Cuckoo Hashing}
\label{sec:pre_cuckoo}
Cuckoo hashing is a technique to resolve hash conflicts in a closed
hashing hash table. Its main draw is that it guarantees constant
lookup times even in densely filled tables.  The distinguishing
technique of cuckoo hashing is, that $H$ hash functions ($h_1, ... ,
h_H$) are used to compute $H$ independent positions. Each element is
stored in one of its positions.  Even if all positions are occupied
one can often move elements to create space for the current
element. We call this process displacing elements.

Bucket cuckoo hashing is a variant, where the cells of the hash table
are grouped into buckets of size $B$ ($m/B$ buckets).  Each element
appointed to one bucket can be stored in any of the bucket's cells .
Using buckets one can drastically increase the number of displacement
opportunities.

\emph{Find} and \emph{remove} operations have a guaranteed constant
running time. Independent from the tables density, there are $H$
buckets -- $H\cdot B$ cells -- that have to be searched to find an
element.

During an \emph{insert} the element is hashed to $H$ buckets.  If at least
one of those buckets has an empty cell, we store the element in the
bucket with the most space.  When all buckets are full, then we
have to move elements around the table, such that a new space becomes
available.

\label{sec:cuckoo_graph}
To visualize the problem of displacing elements, one can think of the
directed graph implicitly defined by the hash table.  Each bucket
defines one node and each element defines pairwise connections between
the bucket it is stored in to its $H-1$ alternate buckets.  To insert
an element into the hash table we have to find a path from one of its
associated buckets, to a bucket, that has space remaining.  Then we
move elements along this path, to open a space on the initial bucket.
The two common techniques to find such paths are \emph{random walks} and
\emph{breadth first searches}.

\section{DySECT Space Efficient Growing, by Splitting the Table}
Our goal is to build an efficiently growing data structure, that
remains space efficient at all times.  A commonly used growing
technique is to double the size by migrating all elements
into a larger new table.  Of course this is
not memory efficient.  A natural thought is to double
only part of the data structure.  This idea together with the
balancing mechanism of cuckoo hashing will achieve the
described functionality.

\subsection{Description}
We use $T$ subtables (shown in TODO FIGREFRENCE) to allow
differentiated growing between table parts.  The subtables consist of
buckets that store elements.

Each element has $H$ associated
buckets similar to cuckoo hashing, these can be in the same, or
in different subtables.  To find a bucket associated with an element $e$,
we compute $e$'s hash value using the appropriate hash function
$h_i(e)$, from a hashed key, we have to compute the subtable and the bucket
within that subtable.

To make this efficient we use powers of two for the number of
subtables ($T = 2^t$), as well as the individual subtable sizes ($s =
2^x*B$).  Since the number of subtables is constant we can use the
first $t$ bits from the hashed key, to find the appropriate subtable.
From the remaining bits we compute the offset into this subtable using
a bitmask ($h_i(e)\,\texttt{\&}\,(s-1) = h_i(e) \mod s$).

%% We split that hash value into two parts, then we use the
%% first part to chose a subtable, and the second part to
%% choose one of the contained buckets.

%% To understand the performance of this hash table, it is important to
%% realize that when the hash table is used regularly, the first level
%% table will always remain cached. This is important, because otherwise lookups
%% would cause unnecessary cache misses that would not have happened in a
%% single level hash table.  Because of this we expect the average number
%% of cache misses during hash table operations to be similar to those of
%% a normal cuckoo hash table.

\subsection{Growing}
We grow one subtable,
when the table contains enough elements, that the memory constraint
can be kept during the migration.  We migrate subtables in order
from the first to the last, therefore, no subtable can be more than
twice as large as any other.

Assume that all subtables have $s=m/T$ cells. When $\alpha\cdot n >
m+2s$ we can grow the first subtable while keeping the size
constraint.  Doubling the size of a subtable increases the global
number of cells from $m_{old} = T\cdot s$ to $m_{new} = m_{old}+s =
(T+1)\cdot s$ (grow factor $\frac{T+1}{T}$).  Note that all subsequent
grows migrate one of the smaller tables, until all tables have the
same size again.  Therefore, each grow until then increase the
capacity by the same absolute amount (smaller grow factor).

The cost of growing the subtable is amortized by all insertions that
happened since the last subtable migration.  There are at least
$\alpha^{-1} \cdot s = \Omega(s)$ insertions between two migrations.
One migration takes $\Theta(s)$ time.  Apart from being amortized,
the migration is efficient in practice because it accesses cells in a
linear fashion making it really cache efficient.  Even in the target
table cells are accessed linearly.  Because of the way we assign
elements to buckets, one original bucket is split into two target
buckets.  Therefore, no displacements are necessary, since no bucket
can be overfilled.

In the implicit graph model of the cuckoo table
(Section~\ref{sec:cuckoo_graph}), growing a subtable is equivalent to
splitting each node that represents a bucket within that table. The
edges (elements) in the implicitly defined subgraph are not doubled,
therefore, the resulting subgraph becomes more sparse, making it
easier to insert elements.

\subsection{Shrinking}
In many use cases, shrinking is not necessary.  It worsens performance
by taking time for the migration and making the remaining table more
dense.  Furthermore, it is unclear that the freed memory is actually
necessary for other parts of the application.  If that memory is
necessary, and shrinking is required, for example, when removed
elements are reinserted into another data structure and global memory
bounds are necessary (other data structures grow while the hash table
shrinks).

Shrinking can work similarly to growing. We replace a subtable with a
smaller one by migrating elements from one to the other.  During this
migration we join elements from two buckets into one. Therefore, it is
possible for a bucket to overfill.  We reinsert these elements at the
end of the migration.  Obviously, this can only apply to at most half
the migrated elements.

When triggering the size reduction, one has to make sure that the
migration is amortized. Therefore, a grow operation cannot immediately
follow a shrink operation.  When shrinking is enabled we propose to
shrink one subtable when $\alpha\cdot n < m-s'$ elements ($s'$ size of a
large table, $m_{new} = m_{old} -s'/2$).

Notice that the allocated memory actually increases during the
migration itself.

\subsection{Implementation Details}
For our experiments in Section~\ref{sec:exp} we use three hash
functions ($H=3$) and a bucket size of ($B=8$). These values have
consistently outperformed other options.  We chose to use $T=256$
subtables for all our tests.  The number of subtables only plays a
minor role for the running time.  To find displacement opportunities
we use breadth first search.  In our tests it performed better than
random walks, since it better uses the read cache lines from one
bucket.

\subsubsection*{Reducing Number Computed Hash Functions}
Evaluating hash functions is expensive, therefore, reducing the number
of hash functions computed per operation increases the efficiency of
the table.  The hash function we use computes 64bit hash values
(i.e. xxHash TODO).  We split the 64bit value into two 32bit hash
values. All common bucket hash table sizes can be addressed using 32
bits. They can address up to $2^{32}$ buckets which achieves a
capacity for $2^{35} \approx 34$ billion elements (bucket size 8),
at that point a hash table consumes 512GiB memory.

When $H > 2$ we can use \emph{double hashing} to further reduce the number
of computed hash functions. Double hashing creates an arbitrary amount
of hash values using only two original hash functions $h'$ and
$h''$.  The additional values are linear combinations computed
from the original two values, $h_i(key) = h'(key) + i\cdot
h''(key)$.

Combining both of these techniques, we can reduce the number of
computed hash functions to one 64bit hash function.  This is
especially important during large displacements, where each
encountered element has to be rehashed to find its alternative
buckets.

\subsubsection*{Table Architecture (two Variants)}
\label{sec:dys_inplace}
The hash table can be implemented as a collection of pointers to
subtables. We have to lookup the corresponding pointer, whenever a
subtable is accessed.  This does not impact performance much since all
subtable pointers will usually be cached.

Another option, which offers some benefits is to use virtual memory,
and memory overcommiting.  The idea is that the operating system will
usually allow larger memory allocations than the machine has memory,
with the anticipation that not all the allocated memory will actually
be used.  Only memory pages, that are actually used will be mapped
from virtual to physical memory pages.  Thus from a strict resource
point of view, the memory is not yet used.  Therefore, we can also
implement the subtables as sections within one huge allocation.  This
has the advantage, that the offset for each table can be quickly
computed, without looking it up from a table.

The added advantage is that we can grow subtables in-place.  To
increase the size of a subtable, it is enough to initialize a
consecutive section of the table (following the original subtable).
Once this is done, we have to redistribute the table's elements.  This
allows us to grow a subtable without the space overhead of
reallocation.

This way, we can also implement in place growing variants of other
hashing schemes (see Section~\ref{sec:exp_competitors}).  These
implementations have the same performance of similar implementations
using full table migration.  But given the correct growing factor,
they can be space efficient.

\subsection{Difficulties for the Analysis of our table structure}
There are two factors, that impact the performance of our dynamic
table compared to other cuckoo table variants and to other hashing
solutions in general \textbf{inhomogeneous table resolution} and
\textbf{element imbalance}.
%, and \textbf{population density}.
All of these factors influence the maximum load density and the
running times in different ways.

\subparagraph*{Imbalance through Inhomogeneous Table Resolution}
\label{sec:inhom_res}
By growing subtables individually we introduce a size imbalance
between subtables.  Large tables contain more buckets, but the number
of elements hashed to the table itself is not dependent on its size,
therefore, it is difficult to spread elements evenly among buckets.
Uneven bucket fill ratios can lead to longer insertion times.

If there are $n$ elements in a hash table with $T$ subtables,
$j$ of which have size $2s$ the others have size $s$. If elements are
spread equally among buckets then all small tables have around
$n/(T+j)$ elements, and the bigger tables have $2n/(T+j)$ elements.
For each table there are about $Hn/T$ elements associated to one of
its buckets.  This shows that having more hash functions can lead to a
better balance.

For two hash functions ($H=2$) and only one grown table ($j=1$) this
means that $\approx 2n/(T+1)$ should be stored in the first table, to
achieve a balanced bucket distribution.  Therefore, nearly all
elements associated to a bucket in the first table ($\approx 2n/T$)
have to be stored there.  This is the reason, why $H=3$ leads to
significantly better space efficiency than $H=2$.

\subparagraph*{Imbalance through Size Changes}
In addition to the problem of inhomogeneous tables, there is an
inherent balancing problem introduced by resizing subtables. It is
clear, that a newly grown table is not filled as densely as other
tables.  Since we double the table size, grown tables can only be
filled to about 50\%.

Assume the global table is filled close to 100\% when the first table
begins to grow.  now there is the capacity for $s$ new elements, but
this capacity is only in the first table, elements that are not hashed
to the first table, automatically trigger displacements leading to
slow insertions.  Notice that repeated operations help to equalize
this imbalance, because elements are more likely inserted into the
less dense areas, and more likely to be deleted from more dense areas.

% \subsubsection*{Population Density}
% Measuring and comparing the performance of space efficient growing
% tables to their statically sized counterparts is difficult.  Inserting
% elements into a densely filled table takes longer, than into an empty
% one.  Therefore, a table that is always densely filled will naturally
% be slower than one that only fills up towards the end of the
% execution -- even when ignoring eventual growing costs.

\section{Experiments}
\label{sec:exp}
There are many factors that impact hash table performance.  Many of
these factors are inherent to the combination of growing and space
efficiency.  To show that our ideas work in practice we use a
portfolio of both micro-benchmarks and practical experiments.

\subsection{Methodology and Comparison Implementations}

\subsubsection*{Comparison Implementations}
\label{sec:exp_competitors}
To put the performance of our solution into perspective, we
implement and test several other options for space efficient
hashing.  We use our own implementations, since no hash table found
online supports strict space-efficiency.

\subparagraph*{Linear Probing} is the most common hashing technique outside of
hashing with chaining.  It is commonly accepted to be inefficient for
higher load factors because search distances can become large.  In
practice, this fact is counteracted by its cache-efficiency making
operations fast in spite of long probing distances.  Linear probing is
especially slow in \verb~find~ and \verb~erase~ operations, that often have
guaranteed constant running time in other implementation.

To grow our linear probing table, we use full table migrations.  This
can be made space efficient, by increasing the capacity in place.
This is possible due to over allocation (similar to
Section~\ref{sec:dys_inplace}) and a simple in place migration
algorithm (APPENDIX OR SECTION?).  For a table that was
initialized with $m$ cells with a min load factor $1-\varepsilon$ we
trigger growing once the table is loaded more than
$1-\frac{\varepsilon}{2}$. We then increase the capacity $m$ to
$(1-\varepsilon)^{-1}\cdot n$ to remain strictly size-efficient.  Our implementation
of the migration is highly cache efficient, but all elements have to
be moved.  This makes repeated migrations inefficient especially for
small $\varepsilon$.

\subparagraph*{Hopscotch Hashing} is a technique similar to linear probing where
elements are moved when they are too far from their original hash
position.  Additionally, hopscotch hashing introduces an acceleration
data structure which uses bitmaps to store neighborhood data. This
data improves find and erase performance.  Many people would expect
the acceleration data to be the main draw to hopscotch hashing, but it
is also its greatest demise in the space-efficient setting.  The
bitmaps introduce a significant memory overhead of neighborhood-size
bits per cell.  Our experiments show that for any reasonable fill
factor neighborhoods have to be 64bits and larger, this introduces a
size factor of $\times1.5$ (128bit elements).  We still include
measurements with hopscotch hashing -- without considering space for
the neighborhood data -- to show its overall performance tendencies.
Growing hopscotch tables is done similar to the technique described
under linear probing.

TODO DESCRIBE 2 CELL BUCKET VARIANT

\subparagraph*{Robin Hood Hashing} is another technique based on linear probing. The
goal of this technique is to improve find performance by minimizing
the maximum displacement distance.  The drawback of this
technique is that insertions have to compute the hash of each element
that is skipped by the insertion.  Migrating a Robin Hood hash table
is very fast, since the additional hash computations are not
necessary, and the element order remains the same (sorted by hash value).

\subparagraph*{Classic Bucket Cuckoo Hashing} as described in
Section~\ref{sec:pre_cuckoo} is the basic hashing technique, that
inspired our hashing technique.  Like our variant it offers constant
lookup and deletion times.  But it suffers from an inefficient growing
technique similar to the one used by the linear probing table above.
Similar to DySECT, $H=3$ and $B=8$ have turned out to be an optimum
for performance.

\subsubsection*{Hardware}
We test our implementation on two very different systems, first a
consumer level desktop computer \ldots{}

\subsection{Influence of Fill Ratio under a Static Table Size}
\label{sec:exp_eps}

The following test was performed, by initializing a table with
$m\approx25M$~cells (non-growing). Then elements are inserted, until there
is a critical amount of failing insertions ($>100$).  At different
stages, we measure the speed of new insertions
(Figure~\ref{fig:eps_insert}), and finds (Figure~\ref{fig:eps_find}).
Performance is measured by performing each operation 1000 times, finds
are measured using either randomly selected elements from within the table (successful), or by
searching random elements from key space (unsuccessful).

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/eps_insert.pdf}
  \caption{\label{fig:eps_insert}Insert timings at different fill
    degrees into a non-growing table.}
\end{figure}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\textwidth]{../eval/plots/eps_find_s.pdf}
  \includegraphics[width=0.49\textwidth]{../eval/plots/eps_find_u.pdf}
  \caption{Timings for find operations with different fill degrees
    (\emph{left:}successful, \emph{right:}unsuccessful).}
  \label{fig:eps_find}
\end{figure}

As one might expect, the performance, of insert operations depends
highly on the fill degree of the table.  Therefore, we show it
normalized with $\frac{1}{\varepsilon}$, which is the expected number
of fully random probes, to find a free cell and thus a natural
estimate for the running time. We see that -- up to a certain point --
for all tables the insertion time behaves proportional to
$\frac{1}{\varepsilon}$, then it begins to decrease uncontrolled.  The
plot shows, that our table remains stable for longer than linear
probing variants, but not as long, as for classic cuckoo hashing.
Hopscotch hashing fails earlier than all other tables, because its
bitmaps become to small, to store the neighborhood information.

DySECT's performance degrades earlier than cuckoo hashing.  One reason
for this is that our hash table consists of $T$ subtables with power
of two sizes and thus does not allow exactly $25M$ cells.  Therefore,
our table is slightly more full.  Another reason is that to get close
to $25M$ cells, we have 125 large subtables (twice the size of
the others).  This introduces inhomogeneous table resolution (see
\ref{sec:inhom_res}), which decreases our performance compared to
cuckoo hashing.

Figure~\ref{fig:eps_find} shows that linear probing variants have bad
performance on find operations on highly filled tables.  It also shows
that unsuccessful find operations are often very slow when using
linear probing.  This is where the acceleration data-structure used by
hopscotch tables can significantly improve performance.  Both of these
effects, do not affect cuckoo variants like DySECT, that have
guaranteed constant running times for all find operations --
independent of the fill degree.

\subsection{Constructing a Table with Growing}
\label{sec:exp_ti}
%% \begin{figure}
%%   \includegraphics[width=0.48\textwidth]{../eval/plots/ti_find_s.pdf}
%%   \includegraphics[width=0.48\textwidth]{../eval/plots/ti_find_u.pdf}
%%   \caption{Timings for find operations on the grown table (from Figure~\ref{fig:ti_insert}; \emph{left:}successful, \emph{right:}unsuccessful).}
%%   \label{fig:ti_find}
%% \end{figure}
In this test $20M$ elements are inserted into a previously empty
table.  The table is initialized expecting 50\,000 elements, thus
growing is necessary, to fit all elements.  The tables are configured,
to guarantee a load factor of at least
$1-\varepsilon$. Figure~\ref{fig:ti_insert} shows the performance in
relation to the load factor.  Insertion times are normalized similar to
Figure~\ref{fig:eps_insert} (divided by $\frac{1}{\varepsilon}$) to
make them more readable.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/ti_insert.pdf}
  \caption{\label{fig:ti_insert}Average time per insert operation
    during the construction of a dynamic table.}
\end{figure}

We see that DySECT performs by far the best even with less filled
tables.  Here we achieve a speedup of TODO$\%$ over the next best
solution (blabla vs. linear probing blabla at $\varepsilon = 0.$ bla).
On denser instances, we can increase this margin to TODO$\%$ (blabla
vs we'll see blabla at $\varepsilon = 0.$ bla).  With shrinking
$\varepsilon$, we see that insertion times start to degrade for our
competitors.  DySECT remains close to $O(\frac{1}{\varepsilon})$ even
for fill degrees up to 97.5$\%$ (TODO check).

We also measured the performance of find operations on the created
tables, they are similar to the performance on the static table in
Section~\ref{sec:exp_eps} (see Figure~\ref{fig:eps_find}), therefore,
we omit displaying them for space reasons.

\subsection{Word Count a Practical use Case}
One of the most common use cases for hash tables -- dynamic hash
tables in particular -- is to unify data elements according to their
key.  In these cases, only dynamic hash tables can remain space
efficient even though the number of unique keys is often unknown.  In
the following test, we take the first block of the common crawl
data-set (TODO REFERENCE) and compute a word count of the contained
words.  The chosen block contains around 240M words, with around 20M
unique words. For the test, we hash each word and insert it's hash
together with a counter.  Subsequent insertions of a re-occurring word
increase this counter.  Similar to the growing benchmark, we start
with an empty table initialized for 50\,000 elements.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{../eval/plots/crawl.pdf}
  \caption{\label{fig:crawl} }
\end{figure}

The performance results can be seen in Figure~\ref{fig:crawl}.  We do
not use any normalization since -- on average -- each word is repeated
12 times, therefore, most inserts will actually behave like successful
find operations (returning an iterator to the contained element).  Our
DySECT table manages to dominate the field, similar to the growing
test in Section~\ref{sec:exp_ti}.  This is a hint to the fact, that
insert performance is important even in find intensive workloads.
This insight is intensified in the following test where we mix insert
and find operations.  (Section~\ref{sec:exp_mix}).


\subsection{Access Patterns/Combining operations}
\label{sec:exp_mix}
In this test, we show how the hash tables behave under mixed
workloads.  We use tables with a minimum load factor of $\varepsilon =
0.95$ (TODO verify).  The test starts a decently filled table
(15M elements).  On this table we perform 10M operations mixed between inserts and
find/erase operations.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\textwidth]{../eval/plots/mix.pdf}
  \includegraphics[width=0.49\textwidth]{../eval/plots/mixd.pdf}
  \caption{\label{fig:mix} }
\end{figure}

This test clearly shows that the
performance of insertions is very important even if there are
significantly more finds than insertions.  This is even more
accentuated by the fact that all performed finds are successful finds,
which have usually significantly better performance than unsuccessful
ones (especially for linear probing; see Figure~\ref{fig:eps_find}).

\subsection{Conclusion}
Our Experiments have shown that our DySECT hash table is by far the
best performing hash table on all instances that make use of its
dynamic capabilities.  It performs up to TODO $\%$ better, than the
next best performing table.


.........


But there remains much to do. We were unable, to prove tight
theoretical performance and density bounds.

Even in practice, there remains a lot of open design space.  One could
use a similar approach to dynamically grow different cuckoo hashing
variants e.g. different variants of concurrent cuckoo
tables~\cite{AlgorithmicImprovementsForFastConcurrentCuckooHashing,
  LockFreeCuckooHashing}; or a table with de-amortized
insertions~\cite{UsingAQueueToDeAmortizeCuckooHashingInHardware,
  DeAmortizedCuckooHashingProvableWorstCasePerformanceAndExperimentalResults}.

............

%%
%% Bibliography
%%

%% Either use bibtex (recommended),

\bibliography{paper}

%% .. or use the thebibliography environment explicitely



\end{document}
